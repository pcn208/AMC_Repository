{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96cfab5b-a512-41e0-85aa-b3e16eaa9d33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchinfo import summary\n",
    "from typing import Tuple, Optional\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d3454ffe-552e-46f8-988f-0f3319a8a0c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0c0c9274-ad02-4fd0-93dd-007b35fec2bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📋 Training Parameters:\n",
      "  Batch size: 256\n",
      "  Epochs: 100\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"C:\\\\workarea\\\\CNN model\\\\dataset\\\\radioml2018\\\\versions\\\\2\\\\GOLD_XYZ_OSC.0001_1024.hdf5\"\n",
    "JSON_PATH = 'C:\\\\workarea\\\\CNN model\\\\dataset\\\\radioml2018\\\\versions\\\\2\\\\classes-fixed.json' \n",
    "\n",
    "TARGET_MODULATIONS = ['4ASK', 'BPSK', 'QPSK', '8PSK','16PSK', '16QAM', '64QAM' ]\n",
    "\n",
    "BATCH_SIZE = 256 # adjust to my laptop \n",
    "#LEARNING_RATE = 0.003 \n",
    "NUM_EPOCHS = 100 \n",
    "NUM_WORKERS = 0 #Temporary check it  \n",
    "\n",
    "INPUT_CHANNELS = 2 \n",
    "SEQUENCE_LENGTH = 1024 \n",
    "NUM_CLASSES = 7 # adjust this to \n",
    "\n",
    "# TRAIN_RATIO = 0.7 \n",
    "# VALID_RATIO = 0.2 \n",
    "# TEST_RATIO = 0.1 \n",
    "\n",
    "nf_train = int(BATCH_SIZE * 0.7)\n",
    "nf_valid = int(BATCH_SIZE * 0.2)\n",
    "nf_test  = BATCH_SIZE - nf_train - nf_valid\n",
    "\n",
    "print(\"📋 Training Parameters:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "#print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75b8b32c-6ee0-4941-9605-8fa3baa37b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(data,\n",
    "                  modulations_classes,\n",
    "                  modulations,\n",
    "                  snrs,\n",
    "                  target_modulations,\n",
    "                  mode,\n",
    "                  target_snrs,\n",
    "                  train_proportion=0.8, # training 70 %\n",
    "                  valid_proportion=0.2, # validation 20 %\n",
    "                  test_proportion=0.0, # testing 10 % \n",
    "                  seed=48):\n",
    "    np.random.seed(seed)\n",
    "    X_output = []\n",
    "    Y_output = []\n",
    "    Z_output = []                                   \n",
    "\n",
    "    target_modulation_indices = [modulations_classes.index(modu) for modu in target_modulations]\n",
    "\n",
    "    for modu in target_modulation_indices:\n",
    "        for snr in target_snrs:\n",
    "            snr_modu_indices = np.where((modulations == modu) & (snrs == snr))[0]\n",
    "\n",
    "            np.random.shuffle(snr_modu_indices)\n",
    "            num_samples = len(snr_modu_indices)\n",
    "            train_end = int(train_proportion * num_samples)\n",
    "            valid_end = int((train_proportion + valid_proportion) * num_samples)\n",
    "\n",
    "            if mode == 'train':\n",
    "                indices = snr_modu_indices[:train_end]\n",
    "            elif mode == 'valid':\n",
    "                indices = snr_modu_indices[train_end:valid_end]\n",
    "            elif mode == 'test':\n",
    "                indices = snr_modu_indices[valid_end:]\n",
    "            else:\n",
    "                raise ValueError(f'unknown mode: {mode}. Valid modes are train, valid and test')\n",
    "\n",
    "            X_output.append(data[np.sort(indices)])\n",
    "            Y_output.append(modulations[np.sort(indices)])\n",
    "            Z_output.append(snrs[np.sort(indices)])\n",
    "\n",
    "    X_array = np.vstack(X_output)\n",
    "    Y_array = np.concatenate(Y_output)\n",
    "    Z_array = np.concatenate(Z_output)\n",
    "    for index, value in enumerate(np.unique(np.copy(Y_array))):\n",
    "        Y_array[Y_array == value] = index\n",
    "    return X_array, Y_array, Z_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2999c7c2-058e-404f-97cc-93b3a25d3bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadioMLIQDataset(Dataset):\n",
    "    \"\"\"Dataset class for RadioML18 data formatted for CNNIQModel dual-branch architecture.\n",
    "    \n",
    "    Loads RadioML18 HDF5 data and returns separate I and Q tensors in 2D format\n",
    "    suitable for CNNIQModel's separate branch processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, mode: str, use_fft: bool = False, seed: int = 48):\n",
    "        \"\"\"Initialize RadioMLIQDataset.\n",
    "        \n",
    "        Args:\n",
    "            mode: Dataset split mode ('train', 'valid', or 'test').\n",
    "            use_fft: Whether to apply FFT transformation to signals.\n",
    "            seed: Random seed for dataset splitting.\n",
    "            \n",
    "        Raises:\n",
    "            FileNotFoundError: If HDF5 or JSON files cannot be found.\n",
    "            ValueError: If mode is not valid or data dimensions are incompatible.\n",
    "        \"\"\"\n",
    "        super(RadioMLIQDataset, self).__init__()\n",
    "        \n",
    "        # Configuration (you'll need to define these constants)\n",
    "        self.file_path = FILE_PATH \n",
    "        self.json_path = JSON_PATH \n",
    "        self.target_modulations = TARGET_MODULATIONS\n",
    "        self.use_fft = use_fft\n",
    "        self.mode = mode\n",
    "        \n",
    "        # Validate mode\n",
    "        if mode not in ['train', 'valid', 'test']:\n",
    "            raise ValueError(f\"Mode must be 'train', 'valid', or 'test', got '{mode}'\")\n",
    "        \n",
    "        # Load data files\n",
    "        try:\n",
    "            self.hdf5_file = h5py.File(self.file_path, 'r')\n",
    "            self.modulation_classes = json.load(open(self.json_path, 'r'))\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Error loading data files: {e}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading file: {e}\")\n",
    "            raise e\n",
    "        \n",
    "        # Load raw data\n",
    "        self.X = self.hdf5_file['X']\n",
    "        self.Y = np.argmax(self.hdf5_file['Y'], axis=1)\n",
    "        self.Z = self.hdf5_file['Z'][:, 0]\n",
    "        \n",
    "        # Calculate proportions for dataset splitting\n",
    "        train_proportion = (7 * 26 * nf_train) / self.X.shape[0]\n",
    "        valid_proportion = (7 * 26 * nf_valid) / self.X.shape[0]\n",
    "        test_proportion = (7 * 26 * nf_test) / self.X.shape[0]\n",
    "        \n",
    "        self.target_snrs = np.unique(self.Z)\n",
    "        \n",
    "        # Split dataset\n",
    "        self.X_data, self.Y_data, self.Z_data = dataset_split(\n",
    "            data=self.X,\n",
    "            modulations_classes=self.modulation_classes,\n",
    "            modulations=self.Y,\n",
    "            snrs=self.Z,\n",
    "            mode=mode,\n",
    "            train_proportion=train_proportion,\n",
    "            valid_proportion=valid_proportion,\n",
    "            test_proportion=test_proportion,\n",
    "            target_modulations=self.target_modulations,\n",
    "            target_snrs=self.target_snrs,\n",
    "            seed=seed\n",
    "        )\n",
    "        \n",
    "        # Apply I/Q swap correction for AMC compatibility\n",
    "        print(f\"🔧 Applying I/Q swap fix to {mode} dataset...\")\n",
    "        self.X_data = self.X_data[:, :, [1, 0]]\n",
    "        print(f\"✅ I/Q channels corrected for real-world compatibility\")\n",
    "        \n",
    "        # Validate signal length for 2D reshaping\n",
    "        signal_length = self.X_data.shape[1]\n",
    "        if signal_length != 1024:\n",
    "            raise ValueError(f\"Expected signal length 1024 for 32x32 reshape, got {signal_length}\")\n",
    "        \n",
    "        if self.use_fft:\n",
    "            print(\"Dataset configured to use FFT as input\")\n",
    "        \n",
    "        # Store dataset statistics\n",
    "        self.num_data = self.X_data.shape[0]\n",
    "        self.num_lbl = len(self.target_modulations)\n",
    "        self.num_snr = self.target_snrs.shape[0]\n",
    "        \n",
    "        print(f\"RadioMLIQDataset {mode}: {self.num_data} samples, \"\n",
    "              f\"{self.num_lbl} classes, {self.num_snr} SNR levels\")\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"Return the number of samples in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Number of samples.\n",
    "        \"\"\"\n",
    "        return self.X_data.shape[0]\n",
    "    \n",
    "    def __getitem__(self, idx: int) -> Tuple[torch.Tensor, torch.Tensor, int, float]:\n",
    "        \"\"\"Get a sample from the dataset.\n",
    "        \n",
    "        Args:\n",
    "            idx: Sample index.\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (i_tensor, q_tensor, label, snr) where:\n",
    "                - i_tensor: I signal as 2D tensor (1, 32, 32)\n",
    "                - q_tensor: Q signal as 2D tensor (1, 32, 32) \n",
    "                - label: Modulation class label\n",
    "                - snr: Signal-to-noise ratio\n",
    "                \n",
    "        Raises:\n",
    "            IndexError: If idx is out of range.\n",
    "        \"\"\"\n",
    "        if idx >= len(self) or idx < 0:\n",
    "            raise IndexError(f\"Index {idx} out of range for dataset of size {len(self)}\")\n",
    "        \n",
    "        # Get raw data\n",
    "        x_raw, y, z = self.X_data[idx], self.Y_data[idx], self.Z_data[idx]\n",
    "        \n",
    "        # Convert to tensor and transpose to (channels, sequence_length)\n",
    "        x = torch.from_numpy(x_raw).float().transpose(0, 1)  # Shape: (2, 1024)\n",
    "        \n",
    "        # Apply FFT if requested\n",
    "        if self.use_fft:\n",
    "            complex_signal = torch.complex(x[0], x[1])\n",
    "            fft_result = torch.fft.fft(complex_signal)\n",
    "            fft_real = torch.real(fft_result)\n",
    "            fft_imag = torch.imag(fft_result)\n",
    "            x = torch.stack([fft_real, fft_imag], dim=0)\n",
    "        \n",
    "        # Separate I and Q signals\n",
    "        i_signal = x[0]  # Shape: (1024,)\n",
    "        q_signal = x[1]  # Shape: (1024,)\n",
    "        \n",
    "        # Reshape 1D signals to 2D (32x32) and add channel dimension\n",
    "        i_2d = i_signal.view(1, 32, 32)  # Shape: (1, 32, 32)\n",
    "        q_2d = q_signal.view(1, 32, 32)  # Shape: (1, 32, 32)\n",
    "        \n",
    "        return i_2d, q_2d, y, z\n",
    "    \n",
    "    def get_signal_stats(self) -> dict:\n",
    "        \"\"\"Get statistics about the signals in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with signal statistics.\n",
    "        \"\"\"\n",
    "        # Sample a few signals to compute stats\n",
    "        sample_indices = np.random.choice(len(self), min(1000, len(self)), replace=False)\n",
    "        i_values = []\n",
    "        q_values = []\n",
    "        \n",
    "        for idx in sample_indices:\n",
    "            i_2d, q_2d, _, _ = self[idx]\n",
    "            i_values.append(i_2d.flatten())\n",
    "            q_values.append(q_2d.flatten())\n",
    "        \n",
    "        i_all = torch.cat(i_values)\n",
    "        q_all = torch.cat(q_values)\n",
    "        \n",
    "        return {\n",
    "            'i_mean': float(i_all.mean()),\n",
    "            'i_std': float(i_all.std()),\n",
    "            'i_min': float(i_all.min()),\n",
    "            'i_max': float(i_all.max()),\n",
    "            'q_mean': float(q_all.mean()),\n",
    "            'q_std': float(q_all.std()),\n",
    "            'q_min': float(q_all.min()),\n",
    "            'q_max': float(q_all.max()),\n",
    "            'signal_shape': '(1, 32, 32)',\n",
    "            'num_samples': len(self)\n",
    "        }\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the HDF5 file handle.\"\"\"\n",
    "        if hasattr(self, 'hdf5_file'):\n",
    "            self.hdf5_file.close()\n",
    "    \n",
    "    def __del__(self):\n",
    "        \"\"\"Cleanup when object is destroyed.\"\"\"\n",
    "        self.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e03fed7e-a55c-433d-9900-67b3bc06e2ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Successfully built CNNIQModel: CNNIQModel(\n",
      "  (i_branch): CNNIQBranch(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (dropout): Dropout2d(p=0.30000000000000004, inplace=False)\n",
      "    (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (q_branch): CNNIQBranch(\n",
      "    (conv1): Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv1_2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn1_2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (conv2_2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (bn2_2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (pool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "    (leaky_relu): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (dropout): Dropout2d(p=0.30000000000000004, inplace=False)\n",
      "    (global_avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=128, out_features=256, bias=True)\n",
      "    (1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (3): Dropout(p=0.4, inplace=False)\n",
      "    (4): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (5): BatchNorm1d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (6): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (7): Dropout(p=0.30000000000000004, inplace=False)\n",
      "    (8): Linear(in_features=128, out_features=64, bias=True)\n",
      "    (9): LeakyReLU(negative_slope=0.1, inplace=True)\n",
      "    (10): Dropout(p=0.2, inplace=False)\n",
      "    (11): Linear(in_features=64, out_features=7, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    from CNN_IQ import create_CNNIQModel\n",
    "\n",
    "    # Match the real signature: (n_labels, dropout_rate)\n",
    "    model_CNNIQ = create_CNNIQModel(n_labels=NUM_CLASSES, dropout_rate=0.4).to(device)\n",
    "    print(\"✅ Successfully built CNNIQModel:\", model_CNNIQ)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"⚠️ Could not build CNNIQModel: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb817dee-b175-496a-8c30-f35dd3cbf27d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\N208\\AppData\\Local\\Temp\\ipykernel_8912\\3942579665.py:12: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = GradScaler()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading datasets...\n",
      "🔧 Applying I/Q swap fix to train dataset...\n",
      "✅ I/Q channels corrected for real-world compatibility\n",
      "RadioMLIQDataset train: 9464 samples, 7 classes, 26 SNR levels\n",
      "Datasets loaded successfully.\n",
      "Total dataset size: 9464\n",
      "Train dataset size: 7571 (80%)\n",
      "Validation dataset size: 1893 (20%)\n",
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|                                           | 0/100 [00:00<?, ?it/s]\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[AC:\\Users\\N208\\AppData\\Local\\Temp\\ipykernel_8912\\3942579665.py:63: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with autocast():\n",
      "\n",
      "Training:  80%|██████████▍  | 24/30 [00:01<00:00, 23.57it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   1%|▎                                  | 1/100 [00:01<02:10,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100: Train Loss: 3.6583 | Train Acc: 21.20% | Valid Loss: 1.8554 | Valid Acc: 29.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   2%|▋                                  | 2/100 [00:02<01:47,  1.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/100: Train Loss: 1.8408 | Train Acc: 26.18% | Valid Loss: 1.6537 | Valid Acc: 29.21%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   3%|█                                  | 3/100 [00:03<01:40,  1.03s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/100: Train Loss: 1.6970 | Train Acc: 27.91% | Valid Loss: 1.6146 | Valid Acc: 30.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   4%|█▍                                 | 4/100 [00:04<01:35,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/100: Train Loss: 1.6622 | Train Acc: 29.96% | Valid Loss: 1.5794 | Valid Acc: 30.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   5%|█▊                                 | 5/100 [00:05<01:33,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/100: Train Loss: 1.6148 | Train Acc: 30.85% | Valid Loss: 1.5522 | Valid Acc: 32.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   6%|██                                 | 6/100 [00:06<01:34,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/100: Train Loss: 1.5971 | Train Acc: 31.18% | Valid Loss: 1.5410 | Valid Acc: 32.12%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   7%|██▍                                | 7/100 [00:07<01:32,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/100: Train Loss: 1.5837 | Train Acc: 32.03% | Valid Loss: 1.5277 | Valid Acc: 32.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   8%|██▊                                | 8/100 [00:08<01:32,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/100: Train Loss: 1.5728 | Train Acc: 31.54% | Valid Loss: 1.5235 | Valid Acc: 32.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:   9%|███▏                               | 9/100 [00:09<01:30,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/100: Train Loss: 1.5548 | Train Acc: 31.95% | Valid Loss: 1.5111 | Valid Acc: 32.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  10%|███▍                              | 10/100 [00:10<01:29,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/100: Train Loss: 1.5456 | Train Acc: 32.53% | Valid Loss: 1.4963 | Valid Acc: 32.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  11%|███▋                              | 11/100 [00:11<01:28,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/100: Train Loss: 1.5312 | Train Acc: 33.22% | Valid Loss: 1.5006 | Valid Acc: 34.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  12%|████                              | 12/100 [00:12<01:27,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/100: Train Loss: 1.5206 | Train Acc: 33.71% | Valid Loss: 1.4872 | Valid Acc: 34.71%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  13%|████▍                             | 13/100 [00:13<01:28,  1.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/100: Train Loss: 1.5110 | Train Acc: 34.18% | Valid Loss: 1.4810 | Valid Acc: 34.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  14%|████▊                             | 14/100 [00:14<01:26,  1.01s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/100: Train Loss: 1.4985 | Train Acc: 34.22% | Valid Loss: 1.4623 | Valid Acc: 36.34%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  15%|█████                             | 15/100 [00:15<01:25,  1.00s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/100: Train Loss: 1.4878 | Train Acc: 35.39% | Valid Loss: 1.4341 | Valid Acc: 37.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  16%|█████▍                            | 16/100 [00:16<01:23,  1.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/100: Train Loss: 1.4703 | Train Acc: 35.64% | Valid Loss: 1.4122 | Valid Acc: 39.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  17%|█████▊                            | 17/100 [00:17<01:22,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/100: Train Loss: 1.4700 | Train Acc: 35.90% | Valid Loss: 1.4050 | Valid Acc: 38.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  18%|██████                            | 18/100 [00:18<01:21,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/100: Train Loss: 1.4521 | Train Acc: 36.57% | Valid Loss: 1.3930 | Valid Acc: 38.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  19%|██████▍                           | 19/100 [00:19<01:19,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/100: Train Loss: 1.4429 | Train Acc: 37.18% | Valid Loss: 1.3834 | Valid Acc: 38.67%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  20%|██████▊                           | 20/100 [00:20<01:19,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/100: Train Loss: 1.4245 | Train Acc: 38.17% | Valid Loss: 1.3646 | Valid Acc: 39.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  21%|███████▏                          | 21/100 [00:21<01:18,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/100: Train Loss: 1.4057 | Train Acc: 37.46% | Valid Loss: 1.3649 | Valid Acc: 40.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  22%|███████▍                          | 22/100 [00:22<01:16,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/100: Train Loss: 1.4014 | Train Acc: 39.04% | Valid Loss: 1.3174 | Valid Acc: 42.47%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  23%|███████▊                          | 23/100 [00:23<01:15,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/100: Train Loss: 1.3986 | Train Acc: 38.42% | Valid Loss: 1.3266 | Valid Acc: 40.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  24%|████████▏                         | 24/100 [00:24<01:15,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/100: Train Loss: 1.3793 | Train Acc: 39.53% | Valid Loss: 1.3090 | Valid Acc: 41.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  25%|████████▌                         | 25/100 [00:25<01:14,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/100: Train Loss: 1.3787 | Train Acc: 39.47% | Valid Loss: 1.3185 | Valid Acc: 42.16%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  26%|████████▊                         | 26/100 [00:26<01:12,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/100: Train Loss: 1.3619 | Train Acc: 39.66% | Valid Loss: 1.3081 | Valid Acc: 41.42%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  27%|█████████▏                        | 27/100 [00:27<01:12,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/100: Train Loss: 1.3520 | Train Acc: 40.71% | Valid Loss: 1.2832 | Valid Acc: 43.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  28%|█████████▌                        | 28/100 [00:27<01:10,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/100: Train Loss: 1.3414 | Train Acc: 40.34% | Valid Loss: 1.3212 | Valid Acc: 40.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  29%|█████████▊                        | 29/100 [00:28<01:10,  1.01it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/100: Train Loss: 1.3556 | Train Acc: 39.51% | Valid Loss: 1.2801 | Valid Acc: 42.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  30%|██████████▏                       | 30/100 [00:29<01:08,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/100: Train Loss: 1.3358 | Train Acc: 40.06% | Valid Loss: 1.2847 | Valid Acc: 42.79%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  31%|██████████▌                       | 31/100 [00:30<01:07,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/100: Train Loss: 1.3300 | Train Acc: 40.60% | Valid Loss: 1.2736 | Valid Acc: 43.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  32%|██████████▉                       | 32/100 [00:31<01:06,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/100: Train Loss: 1.3273 | Train Acc: 40.93% | Valid Loss: 1.2909 | Valid Acc: 41.26%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  33%|███████████▏                      | 33/100 [00:32<01:05,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/100: Train Loss: 1.3194 | Train Acc: 41.34% | Valid Loss: 1.2828 | Valid Acc: 41.63%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A\n",
      "                                                            \u001b[A\n",
      "Validation:   0%|                     | 0/8 [00:00<?, ?it/s]\u001b[A\n",
      "Epochs:  34%|███████████▌                      | 34/100 [00:33<01:04,  1.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/100: Train Loss: 1.3056 | Train Acc: 41.53% | Valid Loss: 1.2802 | Valid Acc: 42.10%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Training:   0%|                      | 0/30 [00:00<?, ?it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "# --- Loss Function and Optimizer ---\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model_CNNIQ.parameters(), lr=0.0015, weight_decay=1e-4)  # Added weight decay\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, \n",
    "    mode='max', \n",
    "    patience=10, \n",
    "    factor=0.5,\n",
    "    #verbose=True\n",
    ")\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "# --- Data Loaders ---\n",
    "print(\"\\nLoading datasets...\")\n",
    "# Load full dataset and split 80% train, 20% validation\n",
    "full_dataset = RadioMLIQDataset('train', use_fft=False)  # Assuming this loads the full dataset\n",
    "dataset_size = len(full_dataset)\n",
    "train_size = int(0.8 * dataset_size)\n",
    "valid_size = dataset_size - train_size\n",
    "\n",
    "# Split dataset randomly\n",
    "train_dataset, valid_dataset = torch.utils.data.random_split(\n",
    "    full_dataset, [train_size, valid_size],\n",
    "    generator=torch.Generator().manual_seed(42)  # For reproducible splits\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, pin_memory=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=BATCH_SIZE, shuffle=False, pin_memory=True)\n",
    "print(\"Datasets loaded successfully.\")\n",
    "print(f\"Total dataset size: {dataset_size}\")\n",
    "print(f\"Train dataset size: {len(train_dataset)} (80%)\")\n",
    "print(f\"Validation dataset size: {len(valid_dataset)} (20%)\")\n",
    "\n",
    "# --- Training and Validation Loop ---\n",
    "train_losses = []\n",
    "valid_losses = []\n",
    "train_accuracies = []  # Store training accuracies\n",
    "valid_accuracies = []\n",
    "best_accuracy = 0.0\n",
    "patience_counter = 0\n",
    "patience = 10\n",
    "BEST_MODEL_PATH = 'best_cnn_model2.pth'\n",
    "\n",
    "# Store final predictions/labels for confusion matrix\n",
    "final_predictions = []\n",
    "final_true_labels = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in tqdm(range(NUM_EPOCHS), desc=\"Epochs\", ncols=80):\n",
    "    model_CNNIQ.train()\n",
    "    running_loss = 0.0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "\n",
    "    # Cleaner progress bar - updates less frequently, no loss display per batch\n",
    "    for i_inputs, q_inputs, labels, _ in tqdm(train_loader, desc=f\"Training\", leave=False, ncols=60, mininterval=1.0):\n",
    "        i_inputs, q_inputs = i_inputs.to(device), q_inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        with autocast():\n",
    "            outputs = model_CNNIQ(i_inputs, q_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        running_loss += loss.item() * i_inputs.size(0)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        total_train += labels.size(0)\n",
    "        correct_train += (predicted == labels).sum().item()\n",
    "\n",
    "    epoch_train_loss = running_loss / len(train_dataset)\n",
    "    epoch_train_accuracy = 100. * correct_train / total_train\n",
    "    train_losses.append(epoch_train_loss)\n",
    "    train_accuracies.append(epoch_train_accuracy)\n",
    "\n",
    "    # --- Validation Phase ---\n",
    "    model_CNNIQ.eval()\n",
    "    validation_loss = 0.0\n",
    "    correct_valid = 0\n",
    "    total_valid = 0\n",
    "    epoch_predictions = []\n",
    "    epoch_true_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i_inputs, q_inputs, labels, _ in tqdm(valid_loader, desc=f\"Validation\", leave=False, ncols=60, mininterval=1.0):\n",
    "            i_inputs, q_inputs = i_inputs.to(device), q_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model_CNNIQ(i_inputs, q_inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            validation_loss += loss.item() * i_inputs.size(0)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            total_valid += labels.size(0)\n",
    "            correct_valid += (predicted == labels).sum().item()\n",
    "            \n",
    "            epoch_predictions.extend(predicted.cpu().numpy())\n",
    "            epoch_true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    epoch_valid_loss = validation_loss / len(valid_dataset)\n",
    "    epoch_valid_accuracy = 100. * correct_valid / total_valid\n",
    "    valid_losses.append(epoch_valid_loss)\n",
    "    valid_accuracies.append(epoch_valid_accuracy)\n",
    "    \n",
    "    scheduler.step(epoch_valid_accuracy)\n",
    "    \n",
    "    # Check for best accuracy and save model\n",
    "    if epoch_valid_accuracy > best_accuracy:\n",
    "        best_accuracy = epoch_valid_accuracy\n",
    "        patience_counter = 0\n",
    "        final_predictions = epoch_predictions\n",
    "        final_true_labels = epoch_true_labels\n",
    "        torch.save(model_CNNIQ.state_dict(), BEST_MODEL_PATH)\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch+1}\")\n",
    "            break\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "          f\"Train Loss: {epoch_train_loss:.4f} | \"\n",
    "          f\"Train Acc: {epoch_train_accuracy:.2f}% | \"\n",
    "          f\"Valid Loss: {epoch_valid_loss:.4f} | \"\n",
    "          f\"Valid Acc: {epoch_valid_accuracy:.2f}%\")\n",
    "\n",
    "print(\"\\nTraining complete.\")\n",
    "print(f\"Best validation accuracy: {best_accuracy:.2f}%\")\n",
    "\n",
    "# --- Plot Training Curves ---\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "# Loss plot\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(valid_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Accuracy plot\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(valid_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy (%)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_curves.png')\n",
    "plt.show()\n",
    "\n",
    "# --- Confusion Matrix for Best Epoch (Run in separate cell) ---\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(final_true_labels, final_predictions)\n",
    "accuracy = 100. * np.diag(cm).sum() / cm.sum()\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=TARGET_MODULATIONS, \n",
    "            yticklabels=TARGET_MODULATIONS)\n",
    "\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.ylabel('True Label')\n",
    "plt.title(f'Confusion Matrix I signal and Q signal process separately(Best Epoch | Overall Accuracy: {accuracy:.2f}%)')\n",
    "plt.xticks(rotation=45)\n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "#plt.savefig('best_confusion_matrix.png')\n",
    "plt.show()\n",
    "\n",
    "# --- Per-Modulation Accuracy Heatmap ---\n",
    "def plot_modulation_snr_accuracy_heatmap(model, dataloader, device, target_modulations):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_true_labels = []\n",
    "    all_snrs = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i_inputs, q_inputs, labels, snrs in dataloader:\n",
    "            i_inputs, q_inputs = i_inputs.to(device), q_inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "            \n",
    "            outputs = model(i_inputs, q_inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            \n",
    "            all_predictions.extend(predicted.cpu().numpy())\n",
    "            all_true_labels.extend(labels.cpu().numpy())\n",
    "            all_snrs.extend(snrs.numpy())\n",
    "    \n",
    "    predictions_df = pd.DataFrame({\n",
    "        'true_label': all_true_labels,\n",
    "        'predicted_label': all_predictions,\n",
    "        'snr': all_snrs\n",
    "    })\n",
    "    \n",
    "    unique_snrs = sorted(predictions_df['snr'].unique())\n",
    "    accuracy_matrix = np.zeros((len(target_modulations), len(unique_snrs)))\n",
    "    \n",
    "    for i, mod in enumerate(target_modulations):\n",
    "        for j, snr in enumerate(unique_snrs):\n",
    "            subset = predictions_df[(predictions_df['true_label'] == i) & (predictions_df['snr'] == snr)]\n",
    "            if len(subset) > 0:\n",
    "                accuracy = (subset['true_label'] == subset['predicted_label']).mean()\n",
    "                accuracy_matrix[i, j] = accuracy * 100\n",
    "    \n",
    "    plt.figure(figsize=(15, 8))\n",
    "    sns.heatmap(accuracy_matrix, \n",
    "                xticklabels=[f'{snr}dB' for snr in unique_snrs],\n",
    "                yticklabels=target_modulations,\n",
    "                annot=True, \n",
    "                fmt='.1f', \n",
    "                cmap='RdYlGn',\n",
    "                vmin=0, \n",
    "                vmax=100,\n",
    "                cbar_kws={'label': 'Accuracy (%)'})\n",
    "    \n",
    "    plt.title('Modulation Classification Accuracy by SNR Level')\n",
    "    plt.xlabel('SNR Level')\n",
    "    plt.ylabel('Modulation Type')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    #plt.savefig('modulation_snr_accuracy_heatm.png')\n",
    "    plt.show()\n",
    "\n",
    "plot_modulation_snr_accuracy_heatmap(model_CNNIQ, valid_loader, device, TARGET_MODULATIONS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f02d0c-5b52-457a-92ae-9c02f504e2d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMC Environtment",
   "language": "python",
   "name": "amc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
