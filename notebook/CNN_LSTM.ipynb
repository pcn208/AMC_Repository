{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3560beac-40a0-4a52-bb68-617aab4c03e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸš€ Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti\n",
      "GPU Memory: 15.9GB\n",
      "âœ… Imports completed successfully!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Check device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"ðŸš€ Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(\"âœ… Imports completed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9bb0d70-c162-47fe-b45d-8bf7e95330a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Successfully imported your CNN_LSTM_parallel.py\n",
      "ðŸŽ¯ Model class ready for use!\n"
     ]
    }
   ],
   "source": [
    "# Method 1: Import your existing model file\n",
    "try:\n",
    "    # Option A: Run your model script directly\n",
    "    %run ./CNN_LSTM_parallel.py\n",
    "    print(\"âœ… Successfully imported your CNN_LSTM_parallel.py\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âš ï¸ Could not run CNN_LSTM_parallel.py: {e}\")\n",
    "    print(\"ðŸ’¡ Using backup model definition...\")\n",
    "    \n",
    "    # Option B: Simple backup model definition (compatible with your original)\n",
    "    class CNN_LSTM_Parallel(nn.Module):\n",
    "        def __init__(self, \n",
    "                     input_channels=2,\n",
    "                     sequence_length=1024,\n",
    "                     num_classes=8,\n",
    "                     cnn_filters=[32, 64, 128],  # Smaller for 4GB VRAM\n",
    "                     lstm_hidden_dim=64,\n",
    "                     lstm_num_layers=1,\n",
    "                     dropout=0.2):\n",
    "            \n",
    "            super(CNN_LSTM_Parallel, self).__init__()\n",
    "            \n",
    "            self.sequence_length = sequence_length\n",
    "            self.lstm_hidden_dim = lstm_hidden_dim\n",
    "            self.num_classes = num_classes\n",
    "            \n",
    "            # Simple CNN branch\n",
    "            self.cnn_kernels = [3, 5]  # Reduced for memory\n",
    "            self.convs = nn.ModuleList([\n",
    "                nn.Sequential(\n",
    "                    nn.Conv1d(input_channels, cnn_filters[0], kernel_size=k, padding=k//2),\n",
    "                    nn.BatchNorm1d(cnn_filters[0]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(cnn_filters[0], cnn_filters[1], kernel_size=k, padding=k//2),\n",
    "                    nn.BatchNorm1d(cnn_filters[1]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.Conv1d(cnn_filters[1], cnn_filters[2], kernel_size=k, padding=k//2),\n",
    "                    nn.BatchNorm1d(cnn_filters[2]),\n",
    "                    nn.ReLU(),\n",
    "                    nn.AdaptiveMaxPool1d(1)\n",
    "                ) for k in self.cnn_kernels\n",
    "            ])\n",
    "            \n",
    "            # Simple LSTM branch\n",
    "            self.lstm = nn.LSTM(\n",
    "                input_size=input_channels,\n",
    "                hidden_size=lstm_hidden_dim,\n",
    "                num_layers=lstm_num_layers,\n",
    "                dropout=dropout if lstm_num_layers > 1 else 0,\n",
    "                batch_first=True,\n",
    "                bidirectional=False  # Simplified for memory\n",
    "            )\n",
    "            \n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "            \n",
    "            # Calculate feature sizes\n",
    "            cnn_feature_size = len(self.cnn_kernels) * cnn_filters[2]  # 2 * 64 = 128\n",
    "            lstm_feature_size = lstm_hidden_dim  # 32\n",
    "            total_features = cnn_feature_size + lstm_feature_size  # 160\n",
    "            \n",
    "            # Simple classifier\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Linear(total_features, total_features // 2),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(dropout),\n",
    "                nn.Linear(total_features // 2, num_classes)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            # CNN branch\n",
    "            cnn_outputs = []\n",
    "            for conv in self.convs:\n",
    "                cnn_out = conv(x).squeeze(-1)\n",
    "                cnn_outputs.append(cnn_out)\n",
    "            \n",
    "            cnn_features = torch.cat(cnn_outputs, dim=1)\n",
    "            cnn_features = self.dropout(cnn_features)\n",
    "            \n",
    "            # LSTM branch\n",
    "            lstm_input = x.transpose(1, 2)\n",
    "            lstm_out, (h_n, c_n) = self.lstm(lstm_input)\n",
    "            lstm_features = h_n[-1]\n",
    "            lstm_features = self.dropout(lstm_features)\n",
    "            \n",
    "            # Combine and classify\n",
    "            combined_features = torch.cat([cnn_features, lstm_features], dim=1)\n",
    "            output = self.classifier(combined_features)\n",
    "            \n",
    "            return output\n",
    "    \n",
    "    print(\"âœ… Backup model definition loaded\")\n",
    "\n",
    "print(\"ðŸŽ¯ Model class ready for use!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c2f211d5-34f0-4cc3-abd2-4b1aaf7d409c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ“‹ Training Parameters:\n",
      "  Batch size: 256\n",
      "  Learning rate: 0.003\n",
      "  Epochs: 100\n",
      "  Classes: 8\n",
      "  Target modulations: ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK']\n",
      "  Device: cuda\n",
      "âœ… Parameters set successfully!\n"
     ]
    }
   ],
   "source": [
    "FILE_PATH = \"C:\\\\workarea\\\\CNN model\\\\dataset\\\\radioml2018\\\\versions\\\\2\\\\GOLD_XYZ_OSC.0001_1024.hdf5\"\n",
    "JSON_PATH = 'C:\\\\workarea\\\\CNN model\\\\dataset\\\\radioml2018\\\\versions\\\\2\\\\classes-fixed.json' \n",
    "\n",
    "TARGET_MODULATIONS = [\n",
    "    'OOK', '4ASK', '8ASK', 'BPSK', \n",
    "    'QPSK', '8PSK', '16PSK', '32PSK'\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 256 \n",
    "LEARNING_RATE = 0.003 \n",
    "NUM_EPOCHS = 100 \n",
    "NUM_WORKERS = 0 #Temporary check it  \n",
    "\n",
    "INPUT_CHANNELS = 2 \n",
    "SEQUENCE_LENGTH = 1024 \n",
    "NUM_CLASSES = 8  \n",
    "\n",
    "TRAIN_RATIO = 0.7 \n",
    "VALID_RATIO = 0.2 \n",
    "TEST_RATIO = 0.1 \n",
    "\n",
    "print(\"ðŸ“‹ Training Parameters:\")\n",
    "print(f\"  Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  Learning rate: {LEARNING_RATE}\")\n",
    "print(f\"  Epochs: {NUM_EPOCHS}\")\n",
    "print(f\"  Classes: {NUM_CLASSES}\")\n",
    "print(f\"  Target modulations: {TARGET_MODULATIONS}\")\n",
    "print(f\"  Device: {device}\")\n",
    "print(\"âœ… Parameters set successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "85e8e3ec-a00b-447c-9835-118c009f6e68",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset function defined\n"
     ]
    }
   ],
   "source": [
    "def split_dataset(data,modulation_classes,modulations, snrs,\n",
    "                 target_modulations,mode,target_snrs,train_ratio = 0.8,valid_ratio = 0.2,test_ratio = 0.0, seed = 48):\n",
    "\n",
    "    X_output = []\n",
    "    Y_output = []\n",
    "    Z_output = []\n",
    "\n",
    "    target_indices = [] \n",
    "    for mod in target_modulations: \n",
    "        if mod in modulation_classes: \n",
    "            idx = modulation_classes.index(mod)\n",
    "            if np.any(modulations == idx): \n",
    "                target_indices.append(idx) \n",
    "    print(f\"found len {len(target_indices)} valid modulations for {mode} split\")\n",
    "\n",
    "    for mod_idx in target_indices:\n",
    "        for snr in target_snrs:\n",
    "            # find samples for this modulation and SNR \n",
    "            indices = np.where((modulations == mod_idx) & (snrs == snr)) [0] \n",
    "            if len(indices) == 0: \n",
    "                continue \n",
    "            #shuffle and split\n",
    "                       \n",
    "            np.random.shuffle(indices)\n",
    "            n_samples = len(indices)\n",
    "            train_end = int(train_ratio * n_samples)\n",
    "            valid_end = int((train_ratio + valid_ratio) * n_samples)\n",
    "            \n",
    "            if mode == 'train': \n",
    "                selected_indices = indices[:train_end] \n",
    "            elif mode == 'valid': \n",
    "                selected_indices = indices[train_end:valid_end]\n",
    "            elif mode == 'test':\n",
    "                selected_indices = indices[valid_end:]\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "            if len(selected_indices) > 0: \n",
    "                X_output.append(data[np.sort(selected_indices)])\n",
    "                Y_output.append(modulations[np.sort(selected_indices)]) \n",
    "                Z_output.append(snrs[np.sort(selected_indices)]) \n",
    "    if len(X_output) == 0: \n",
    "        print(f\"No data found for {mode} split !\") \n",
    "        return np.array([]), np.array([]), np.array([]) \n",
    "\n",
    "    #COmbine all the data \n",
    "    X_array = np.vstack(X_output)\n",
    "    Y_array = np.concatenate(Y_output)\n",
    "    Z_array = np.concatenate(Z_output)\n",
    "\n",
    "    unique_labels = np.unique(Y_array) \n",
    "    for i, label in enumerate(unique_labels): \n",
    "        Y_array[Y_array == label] = i \n",
    "    print(f\"{mode.capitalize()} split: {len(X_array):,} samples\") \n",
    "    return X_array,Y_array, Z_array\n",
    "print(\"Dataset function defined\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6402878a-1e84-4f72-82e9-ea60815c67c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset defined\n"
     ]
    }
   ],
   "source": [
    "class RadioML18Dataset(Dataset): \n",
    "    def __init__(self, mode = 'train', target_modulations = None, file_path = None, json_path = None): \n",
    "        super (RadioML18Dataset, self).__init__()\n",
    "\n",
    "        #use global parameter if not provided \n",
    "        self.file_path = file_path or FILE_PATH \n",
    "        self.json_path = json_path or JSON_PATH \n",
    "        self.target_modulations = target_modulations or TARGET_MODULATIONS \n",
    "\n",
    "        print(f\"loading {mode} dataset .. \") \n",
    "        print(f\"Target modulation : {self.target_modulations}\") \n",
    "\n",
    "        #load data files \n",
    "        try: \n",
    "            self.hdf5_file = h5py.File(self.file_path,'r') \n",
    "            self.modulation_classes = json.load(open(self.json_path,'r')) \n",
    "            print(\"File loaded Succesfully\")\n",
    "        except Exception as e: \n",
    "            print(f\"Error loading files : {e}\") \n",
    "            raise e \n",
    "\n",
    "        #load array \n",
    "        self.X = self.hdf5_file['X'] \n",
    "        self.Y = np.argmax(self.hdf5_file['Y'], axis = 1) \n",
    "        self.Z = self.hdf5_file['Z'][:,0] \n",
    "        self.target_snrs = np.unique(self.Z) \n",
    "\n",
    "        print(f\"Total dataset : {self.X.shape[0]:,} samples and for SNR Range {self.target_snrs.min()} - {self.target_snrs.max()} dB \")\n",
    "\n",
    "        #split dataset \n",
    "        self.X_data,self.Y_data,self.Z_data = split_dataset(\n",
    "            data = self.X, \n",
    "            modulation_classes= self.modulation_classes, \n",
    "            modulations=self.Y, \n",
    "            snrs = self.Z, \n",
    "            target_modulations = self.target_modulations, \n",
    "            mode = mode, \n",
    "            target_snrs=self.target_snrs, \n",
    "            train_ratio = TRAIN_RATIO, \n",
    "            valid_ratio = VALID_RATIO, \n",
    "            test_ratio =  TEST_RATIO\n",
    "        )\n",
    "\n",
    "        if len(self.X_data) == 0: \n",
    "            print(f\"No data found for {mode} split! \")\n",
    "            self.hdf5_file.close()\n",
    "            return \n",
    "        \n",
    "        ### Do IQ correction \n",
    "        X_corrected = np.zeros_like(self.X_data)\n",
    "        X_corrected[:, :, 0] = self.X_data[:, :, 1]\n",
    "        X_corrected[:, :, 1] = self.X_data[:, :, 0]\n",
    "        self.X_data = X_corrected \n",
    "\n",
    "        self.num_samples = len(self.X_data)\n",
    "        self.num_classes = len(self.target_modulations)\n",
    "        self.num_snrs = len(self.target_snrs)\n",
    "\n",
    "        print(f\"Ready!{mode.capitalize()}, Sampels: {self.num_samples}, classes: {self.num_classes}, SNRS: {self.num_snrs}\")\n",
    "        \n",
    "        #close file to save memory usage \n",
    "        self.hdf5_file.close()\n",
    "\n",
    "    def __len__(self): \n",
    "        return len(self.X_data) if hasattr(self,'X_data') else 0 \n",
    "    \n",
    "    def __getitem__(self,idx): \n",
    "        if len(self.X_data) == 0: \n",
    "            raise IndexError(\"Dataset is empty\")\n",
    "        x = torch.FloatTensor(self.X_data[idx]).transpose(0,1)\n",
    "        y = torch.LongTensor([self.Y_data[idx]]).squeeze()\n",
    "        z = torch.LongTensor([self.Z_data[idx]]).squeeze()\n",
    "\n",
    "        return x,y,z \n",
    "print(\"dataset defined\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b67f3c1f-1386-470e-b2ae-f0db69dabf60",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simpel_model():\n",
    "    print(\"Creating model ...\")\n",
    "\n",
    "    model = CNN_LSTM_Parallel(\n",
    "        input_channels= INPUT_CHANNELS,\n",
    "        sequence_length= SEQUENCE_LENGTH,\n",
    "        num_classes=NUM_CLASSES,\n",
    "        cnn_filters=[64,128 , 256] , # smaller filters for 4 gb of vram\n",
    "        lstm_hidden_dim=128, # SMALLER LSTM\n",
    "        lstm_num_layers=3, # single layer\n",
    "        dropout=0.2\n",
    "    )\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "    print(f\"Total parameter is {total_params:,} \\n Trainable parameter: {trainable_params:,} \\n model size: ~{total_params * 4/1024**2:.1f} MB\")\n",
    "\n",
    "    print(\"testing model \")\n",
    "    sample_input = torch.randn(1,2,1024).to(device)\n",
    "\n",
    "    # Penting: Set model ke mode evaluasi untuk pengujian satu sampel\n",
    "    model.eval() # <--- Tambahkan baris ini\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(sample_input)\n",
    "        print(f\" Input shape : {sample_input.shape} \\n output shape : {output.shape}, \\n output classes : {output.shape[1]}\")\n",
    "\n",
    "    print(\"model created successfully \")\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ddb1ddd9-5ecf-4f7e-b733-15f83d1d4d90",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping: \n",
    "    def __init__(self,patience = 15, min_delta = 0.001, restore_best_weights = True): \n",
    "        self.patience = patience \n",
    "        self.min_delta = min_delta \n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = float('inf') \n",
    "        self.counter = 0 \n",
    "        self.best_weights = None\n",
    "        \n",
    "    def __call__(self, val_loss,model):\n",
    "        if val_loss < self.best_loss - self.min_delta: \n",
    "            self.best_loss = val_loss \n",
    "            self.counter = 0 \n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy\n",
    "            else: \n",
    "                self.counter += 1 \n",
    "            if self.counter >= self.patience: \n",
    "                if self.restore_best_weights and self.best_weigths is not None: \n",
    "                    model.load_state_dict(self.best_weights) \n",
    "                return True \n",
    "            return False\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "996c1ed7-b2ed-4454-a974-84ee39ddd43d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating model ...\n",
      "Total parameter is 871,496 \n",
      " Trainable parameter: 871,496 \n",
      " model size: ~3.3 MB\n",
      "testing model \n",
      " Input shape : torch.Size([1, 2, 1024]) \n",
      " output shape : torch.Size([1, 8]), \n",
      " output classes : 8\n",
      "model created successfully \n"
     ]
    }
   ],
   "source": [
    "# Test model creation\n",
    "try:\n",
    "    model = simpel_model().to('cuda')\n",
    "    model.eval()\n",
    "    # Clean up test tensors\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error creating model: {e}\")\n",
    "    print(\"ðŸ’¡ Make sure your CNN_LSTM_parallel.py is compatible\")\n",
    "    raise e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ad1af89d-915a-4949-a252-cb7c3616658e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, criterion, optimizer, scaler, device): \n",
    "    model.train()\n",
    "    total_loss = 0.0 \n",
    "    correct = 0\n",
    "    total = 0 \n",
    "\n",
    "    progress_bar = tqdm(dataloader, desc=\"Training\", leave=False)\n",
    "\n",
    "    for batch_idx, (data, target, snr) in enumerate(progress_bar): \n",
    "        data = data.to(device, non_blocking=True)  # Fixed typo: non_blockig -> non_blocking\n",
    "        target = target.to(device, non_blocking=True)\n",
    "\n",
    "        optimizer.zero_grad()  # Fixed: zero.grad() -> zero_grad()\n",
    "\n",
    "        with autocast(): \n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "        accuracy = 100.0 * correct / total \n",
    "        # Fixed: Use keyword arguments instead of colon syntax\n",
    "        progress_bar.set_postfix(\n",
    "            Loss=f'{loss.item():.4f}',\n",
    "            Acc=f'{accuracy:.2f}%'\n",
    "        )\n",
    "\n",
    "        if batch_idx % 10 == 0: \n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100.0 * correct / total \n",
    "\n",
    "    return avg_loss, accuracy \n",
    "\n",
    "def validate_one_epoch(model, dataloader, criterion, device): \n",
    "    model.eval()\n",
    "    total_loss = 0.0 \n",
    "    correct = 0 \n",
    "    total = 0 \n",
    "    \n",
    "    with torch.no_grad(): \n",
    "        progress_bar = tqdm(dataloader, desc=\"Validating\", leave=False)\n",
    "\n",
    "        for data, target, snr in progress_bar: \n",
    "            data = data.to(device, non_blocking=True)\n",
    "            target = target.to(device, non_blocking=True)\n",
    "\n",
    "            with autocast(): \n",
    "                output = model(data)\n",
    "                loss = criterion(output, target)\n",
    "\n",
    "            total_loss += loss.item()\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            total += target.size(0)\n",
    "            correct += (predicted == target).sum().item()\n",
    "\n",
    "            accuracy = 100.0 * correct / total \n",
    "            # Fixed: Use keyword arguments\n",
    "            progress_bar.set_postfix(\n",
    "                Loss=f'{loss.item():.4f}',\n",
    "                Acc=f'{accuracy:.2f}%'\n",
    "            )\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = 100.0 * correct / total \n",
    "\n",
    "    return avg_loss, accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cd70bebb-fb90-450c-b548-a324f9d1a938",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_results(train_losses,val_losses, train_accs, val_accs): \n",
    "\n",
    "    fig, (ax1,ax2,ax3) = plt.subplots(1,2,3, fig_size = (12,4))\n",
    "\n",
    "    #plot losses \n",
    "\n",
    "    ax1.plot(train_losses, label = 'Training Loss', color = 'blue')\n",
    "    ax1.plot(val_losses, label = 'Validating loss', color = 'red')\n",
    "    ax1.set_title('Training and validaing loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True)\n",
    "\n",
    "    #Plot Accuracies \n",
    "    ax2.plot(train_accs, label='Training Accuracy', color='blue')\n",
    "    ax2.plot(val_accs, label='Validation Accuracy', color='red')\n",
    "    ax2.set_title('Training and Validation Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True)    \n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "153fdfd5-7ef1-477c-b158-5bbc9576fba1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”¥ Ready to start training!\n",
      "ðŸ’¡ Run: results = run_training()\n",
      "   or just execute this cell to start training immediately\n"
     ]
    }
   ],
   "source": [
    "def run_training(): \n",
    "\n",
    "    train_dataset = RadioML18Dataset(mode = 'train')\n",
    "    valid_dataset = RadioML18Dataset(mode = 'valid')\n",
    "    test_dataset = RadioML18Dataset(mode = 'test')\n",
    "\n",
    "    if len(train_dataset) == 0:\n",
    "        print(\"Traingin is empty ! \")\n",
    "        return None \n",
    "    \n",
    "    train_loader = DataLoader(\n",
    "        train_dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        shuffle=True, \n",
    "        num_workers=NUM_WORKERS, \n",
    "        pin_memory=True, \n",
    "        drop_last = True \n",
    "    )\n",
    "\n",
    "    valid_loader = DataLoader(\n",
    "        valid_dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS, \n",
    "        pin_memory=True, \n",
    "        #drop_last = True \n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset, \n",
    "        batch_size = BATCH_SIZE, \n",
    "        shuffle=False, \n",
    "        num_workers=NUM_WORKERS, \n",
    "        pin_memory=True, \n",
    "        #drop_last = True \n",
    "    )\n",
    "\n",
    "    print(f\"Train batches : {len(train_loader)} \\n Valid batches : {len(valid_loader)} \\n Test batches : {len(test_loader)}\")\n",
    "\n",
    "    print(\"Setting up Training\")\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.5)\n",
    "    scaler = GradScaler() \n",
    "\n",
    "    # Training history\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    val_losses = []\n",
    "    val_accuracies = []\n",
    "    \n",
    "    best_val_acc = 0.0   \n",
    "\n",
    "    print(f\"âœ… Training setup complete\")\n",
    "    print(f\"  Optimizer: Adam (lr={LEARNING_RATE})\")\n",
    "    print(f\"  Loss function: CrossEntropyLoss\")\n",
    "    print(f\"  Scheduler: StepLR (step=10, gamma=0.5)\")\n",
    "    \n",
    "    # Step 4: Training loop\n",
    "    print(f\"\\nðŸ‹ï¸ Step 4: Training for {NUM_EPOCHS} epochs...\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS}\")\n",
    "        print(\"-\" * 30)\n",
    "        \n",
    "        # Train\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, train_loader, criterion, optimizer, scaler, device\n",
    "        )\n",
    "        \n",
    "        # Validate\n",
    "        val_loss, val_acc = validate_one_epoch(\n",
    "            model, valid_loader, criterion, device\n",
    "        )\n",
    "        \n",
    "        # Update scheduler\n",
    "        scheduler.step()\n",
    "        \n",
    "        # Save history\n",
    "        train_losses.append(train_loss)\n",
    "        train_accuracies.append(train_acc)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        \n",
    "        # Print results\n",
    "        print(f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.2f}%\")\n",
    "        print(f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.2f}%\")\n",
    "        \n",
    "        # Save best model\n",
    "        if val_acc > best_val_acc:\n",
    "            best_val_acc = val_acc\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': model.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'best_val_acc': best_val_acc,\n",
    "            }, 'best_model_simple.pth')\n",
    "            print(f\"ðŸ’¾ New best model saved! (Val Acc: {val_acc:.2f}%)\")\n",
    "        \n",
    "        # Memory cleanup\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "    \n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"\\nâ±ï¸ Training completed in {training_time/60:.2f} minutes\")\n",
    "    \n",
    "    # Step 5: Test the best model\n",
    "    print(\"\\nðŸ§ª Step 5: Testing best model...\")\n",
    "    \n",
    "    # Load best model\n",
    "    checkpoint = torch.load('best_model_simple.pth', map_location=device)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    # Test\n",
    "    test_loss, test_acc = validate_one_epoch(model, test_loader, criterion, device)\n",
    "    \n",
    "    print(f\"ðŸŽ¯ Final Results:\")\n",
    "    print(f\"  Best Validation Accuracy: {best_val_acc:.2f}%\")\n",
    "    print(f\"  Test Accuracy: {test_acc:.2f}%\")\n",
    "    print(f\"  Test Loss: {test_loss:.4f}\")\n",
    "    \n",
    "    # Step 6: Plot results\n",
    "    print(\"\\nðŸ“ˆ Step 6: Plotting results...\")\n",
    "    plot_training_results(train_losses, val_losses, train_accuracies, val_accuracies)\n",
    "    \n",
    "    results = {\n",
    "        'train_losses': train_losses,\n",
    "        'val_losses': val_losses,\n",
    "        'train_accuracies': train_accuracies,\n",
    "        'val_accuracies': val_accuracies,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'test_acc': test_acc,\n",
    "        'training_time': training_time\n",
    "    }\n",
    "    \n",
    "    print(\"ðŸŽ‰ Training pipeline completed successfully!\")\n",
    "    return results\n",
    "\n",
    "# Run the training (uncomment to start)\n",
    "print(\"ðŸ”¥ Ready to start training!\")\n",
    "print(\"ðŸ’¡ Run: results = run_training()\")\n",
    "print(\"   or just execute this cell to start training immediately\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d804a2f6-f5f7-482f-a6fa-8c7696386324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model_performance():\n",
    "    \"\"\"\n",
    "    Detailed testing and visualization of model performance\n",
    "    \"\"\"\n",
    "    print(\"ðŸ§ª Detailed Model Testing\")\n",
    "    print(\"=\" * 40)\n",
    "    \n",
    "    # Load test dataset\n",
    "    test_dataset = RadioML18Dataset(mode='test')\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "    # Load best model\n",
    "    try:\n",
    "        checkpoint = torch.load('best_model_simple.pth', map_location=device)\n",
    "        model.load_state_dict(checkpoint['model_state_dict'])\n",
    "        print(\"âœ… Best model loaded\")\n",
    "    except:\n",
    "        print(\"âš ï¸ No saved model found, using current model\")\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Collect predictions\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "    all_snrs = []\n",
    "    \n",
    "    print(\"ðŸ” Collecting predictions...\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for data, target, snr in tqdm(test_loader, desc=\"Testing\"):\n",
    "            data = data.to(device)\n",
    "            output = model(data)\n",
    "            \n",
    "            predictions = torch.argmax(output, dim=1).cpu().numpy()\n",
    "            targets = target.numpy()\n",
    "            snrs = snr.numpy()\n",
    "            \n",
    "            all_predictions.extend(predictions)\n",
    "            all_targets.extend(targets)\n",
    "            all_snrs.extend(snrs)\n",
    "    \n",
    "    all_predictions = np.array(all_predictions)\n",
    "    all_targets = np.array(all_targets)\n",
    "    all_snrs = np.array(all_snrs)\n",
    "    \n",
    "    # Calculate overall accuracy\n",
    "    overall_accuracy = np.mean(all_predictions == all_targets) * 100\n",
    "    print(f\"ðŸŽ¯ Overall Test Accuracy: {overall_accuracy:.2f}%\")\n",
    "    \n",
    "    # Accuracy per SNR\n",
    "    unique_snrs = np.unique(all_snrs)\n",
    "    snr_accuracies = []\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Accuracy per SNR:\")\n",
    "    for snr in sorted(unique_snrs):\n",
    "        mask = all_snrs == snr\n",
    "        if np.sum(mask) > 0:\n",
    "            acc = np.mean(all_predictions[mask] == all_targets[mask]) * 100\n",
    "            snr_accuracies.append(acc)\n",
    "            print(f\"  SNR {snr:3.0f} dB: {acc:5.2f}%\")\n",
    "        else:\n",
    "            snr_accuracies.append(0)\n",
    "    \n",
    "    # Plot accuracy vs SNR\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(sorted(unique_snrs), snr_accuracies, 'bo-', linewidth=2, markersize=6)\n",
    "    plt.xlabel('SNR (dB)')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.title('Classification Accuracy vs SNR')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.ylim(0, 100)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    cm = confusion_matrix(all_targets, all_predictions)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=TARGET_MODULATIONS,\n",
    "                yticklabels=TARGET_MODULATIONS)\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('Actual')\n",
    "    plt.title('Confusion Matrix')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Classification report\n",
    "    print(f\"\\nðŸ“‹ Classification Report:\")\n",
    "    from sklearn.metrics import classification_report\n",
    "    report = classification_report(all_targets, all_predictions, \n",
    "                                   target_names=TARGET_MODULATIONS)\n",
    "    print(report)\n",
    "    \n",
    "    return {\n",
    "        'overall_accuracy': overall_accuracy,\n",
    "        'snr_accuracies': dict(zip(sorted(unique_snrs), snr_accuracies)),\n",
    "        'confusion_matrix': cm,\n",
    "        'predictions': all_predictions,\n",
    "        'targets': all_targets,\n",
    "        'snrs': all_snrs\n",
    "    }\n",
    "\n",
    "def quick_test():\n",
    "    \"\"\"\n",
    "    Quick test to verify everything is working\n",
    "    \"\"\"\n",
    "    print(\"âš¡ Quick Test\")\n",
    "    print(\"=\" * 20)\n",
    "    \n",
    "    # Create a small test dataset\n",
    "    test_dataset = RadioML18Dataset(mode='test')\n",
    "    \n",
    "    if len(test_dataset) == 0:\n",
    "        print(\"âŒ No test data available\")\n",
    "        return\n",
    "    \n",
    "    # Test a few samples\n",
    "    print(f\"ðŸ“Š Test dataset: {len(test_dataset):,} samples\")\n",
    "    \n",
    "    # Get a sample\n",
    "    sample_x, sample_y, sample_z = test_dataset[0]\n",
    "    print(f\"Sample shape: {sample_x.shape}\")\n",
    "    print(f\"Sample label: {sample_y} ({TARGET_MODULATIONS[sample_y]})\")\n",
    "    print(f\"Sample SNR: {sample_z} dB\")\n",
    "    \n",
    "    # Test model prediction\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        sample_x = sample_x.unsqueeze(0).to(device)  # Add batch dimension\n",
    "        output = model(sample_x)\n",
    "        prediction = torch.argmax(output, dim=1).item()\n",
    "        confidence = torch.softmax(output, dim=1).max().item()\n",
    "    \n",
    "    print(f\"Model prediction: {prediction} ({TARGET_MODULATIONS[prediction]})\")\n",
    "    print(f\"Confidence: {confidence:.3f}\")\n",
    "    print(f\"Correct: {'âœ…' if prediction == sample_y else 'âŒ'}\")\n",
    "    \n",
    "    print(\"âœ… Quick test completed!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "24b80829-8cfe-48bb-8486-0065610a4800",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âš¡ Quick Test\n",
      "====================\n",
      "loading test dataset .. \n",
      "Target modulation : ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK']\n",
      "File loaded Succesfully\n",
      "Total dataset : 2,555,904 samples and for SNR Range -20 - 30 dB \n",
      "found len 8 valid modulations for test split\n",
      "No data found for test split !\n",
      "No data found for test split! \n",
      "âŒ No test data available\n"
     ]
    }
   ],
   "source": [
    "quick_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7430f9d4-0615-4493-aa27-33ffe9774c8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading train dataset .. \n",
      "Target modulation : ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK']\n",
      "File loaded Succesfully\n",
      "Total dataset : 2,555,904 samples and for SNR Range -20 - 30 dB \n",
      "found len 8 valid modulations for train split\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m result = \u001b[43mrun_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 3\u001b[39m, in \u001b[36mrun_training\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrun_training\u001b[39m(): \n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m     train_dataset = \u001b[43mRadioML18Dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtrain\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      4\u001b[39m     valid_dataset = RadioML18Dataset(mode = \u001b[33m'\u001b[39m\u001b[33mvalid\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m      5\u001b[39m     test_dataset = RadioML18Dataset(mode = \u001b[33m'\u001b[39m\u001b[33mtest\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 31\u001b[39m, in \u001b[36mRadioML18Dataset.__init__\u001b[39m\u001b[34m(self, mode, target_modulations, file_path, json_path)\u001b[39m\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal dataset : \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.X.shape[\u001b[32m0\u001b[39m]\u001b[38;5;132;01m:\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m samples and for SNR Range \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.target_snrs.min()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.target_snrs.max()\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m dB \u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m \u001b[38;5;66;03m#split dataset \u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m \u001b[38;5;28mself\u001b[39m.X_data,\u001b[38;5;28mself\u001b[39m.Y_data,\u001b[38;5;28mself\u001b[39m.Z_data = \u001b[43msplit_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodulation_classes\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodulation_classes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     34\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodulations\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     35\u001b[39m \u001b[43m    \u001b[49m\u001b[43msnrs\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mZ\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     36\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_modulations\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_modulations\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     37\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     38\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtarget_snrs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtarget_snrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     39\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mTRAIN_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     40\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalid_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mVALID_RATIO\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     41\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_ratio\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m  \u001b[49m\u001b[43mTEST_RATIO\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m.X_data) == \u001b[32m0\u001b[39m: \n\u001b[32m     45\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mNo data found for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m split! \u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 39\u001b[39m, in \u001b[36msplit_dataset\u001b[39m\u001b[34m(data, modulation_classes, modulations, snrs, target_modulations, mode, target_snrs, train_ratio, valid_ratio, test_ratio, seed)\u001b[39m\n\u001b[32m     36\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnknown mode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m     38\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(selected_indices) > \u001b[32m0\u001b[39m: \n\u001b[32m---> \u001b[39m\u001b[32m39\u001b[39m     X_output.append(\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43msort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mselected_indices\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[32m     40\u001b[39m     Y_output.append(modulations[np.sort(selected_indices)]) \n\u001b[32m     41\u001b[39m     Z_output.append(snrs[np.sort(selected_indices)]) \n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:56\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mh5py/_objects.pyx:57\u001b[39m, in \u001b[36mh5py._objects.with_phil.wrapper\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\.conda\\envs\\amc_env\\Lib\\site-packages\\h5py\\_hl\\dataset.py:820\u001b[39m, in \u001b[36mDataset.__getitem__\u001b[39m\u001b[34m(self, args, new_dtype)\u001b[39m\n\u001b[32m    818\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m._fast_read_ok \u001b[38;5;129;01mand\u001b[39;00m (new_dtype \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    819\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m820\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fast_reader\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    821\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m    822\u001b[39m         \u001b[38;5;28;01mpass\u001b[39;00m  \u001b[38;5;66;03m# Fall back to Python read pathway below\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "result = run_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b49d75-0104-4302-bb3a-9160307bc62c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "AMC Environtment",
   "language": "python",
   "name": "amc_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
