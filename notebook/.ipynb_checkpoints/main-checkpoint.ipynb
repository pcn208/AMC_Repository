{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aed6ae17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from pandas) (2.1.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
      "Requirement already satisfied: h5py in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (3.14.0)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from h5py) (2.1.2)\n",
      "Requirement already satisfied: tqdm in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from tqdm) (0.4.6)\n",
      "Requirement already satisfied: torchinfo in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (1.8.0)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (2.1.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: seaborn in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: numpy!=1.24.0,>=1.20 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from seaborn) (2.1.2)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from seaborn) (2.3.0)\n",
      "Requirement already satisfied: matplotlib!=3.6.1,>=3.4 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from seaborn) (3.10.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (4.58.4)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (1.4.8)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from matplotlib!=3.6.1,>=3.4->seaborn) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from python-dateutil>=2.7->matplotlib!=3.6.1,>=3.4->seaborn) (1.17.0)\n",
      "Collecting scikit-learn\n",
      "  Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl.metadata (14 kB)\n",
      "Requirement already satisfied: numpy>=1.22.0 in c:\\users\\n208\\.conda\\envs\\amc_env\\lib\\site-packages (from scikit-learn) (2.1.2)\n",
      "Collecting scipy>=1.8.0 (from scikit-learn)\n",
      "  Downloading scipy-1.15.3-cp312-cp312-win_amd64.whl.metadata (60 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Downloading joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Downloading threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Downloading scikit_learn-1.7.0-cp312-cp312-win_amd64.whl (10.7 MB)\n",
      "   ---------------------------------------- 0.0/10.7 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/10.7 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.8/10.7 MB 2.4 MB/s eta 0:00:05\n",
      "   ---- ----------------------------------- 1.3/10.7 MB 2.5 MB/s eta 0:00:04\n",
      "   ------- -------------------------------- 2.1/10.7 MB 2.7 MB/s eta 0:00:04\n",
      "   --------- ------------------------------ 2.6/10.7 MB 2.7 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 3.1/10.7 MB 2.7 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.9/10.7 MB 2.8 MB/s eta 0:00:03\n",
      "   ----------------- ---------------------- 4.7/10.7 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------------- -------------------- 5.2/10.7 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 6.0/10.7 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 6.8/10.7 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 7.3/10.7 MB 3.1 MB/s eta 0:00:02\n",
      "   ---------------------------- ----------- 7.6/10.7 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------------ --------- 8.1/10.7 MB 2.9 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 8.7/10.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 9.4/10.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 10.0/10.7 MB 2.9 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 10.7/10.7 MB 2.9 MB/s eta 0:00:00\n",
      "Downloading joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Downloading scipy-1.15.3-cp312-cp312-win_amd64.whl (41.0 MB)\n",
      "   ---------------------------------------- 0.0/41.0 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/41.0 MB 2.8 MB/s eta 0:00:15\n",
      "   - -------------------------------------- 1.0/41.0 MB 3.0 MB/s eta 0:00:14\n",
      "   - -------------------------------------- 1.8/41.0 MB 3.0 MB/s eta 0:00:13\n",
      "   -- ------------------------------------- 2.4/41.0 MB 3.1 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 3.1/41.0 MB 3.1 MB/s eta 0:00:13\n",
      "   --- ------------------------------------ 3.7/41.0 MB 3.2 MB/s eta 0:00:12\n",
      "   ---- ----------------------------------- 4.5/41.0 MB 3.2 MB/s eta 0:00:12\n",
      "   ----- ---------------------------------- 5.2/41.0 MB 3.3 MB/s eta 0:00:11\n",
      "   ----- ---------------------------------- 6.0/41.0 MB 3.4 MB/s eta 0:00:11\n",
      "   ------ --------------------------------- 7.1/41.0 MB 3.5 MB/s eta 0:00:10\n",
      "   ------- -------------------------------- 8.1/41.0 MB 3.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 8.9/41.0 MB 3.7 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 8.9/41.0 MB 3.7 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 9.4/41.0 MB 3.5 MB/s eta 0:00:09\n",
      "   --------- ------------------------------ 10.0/41.0 MB 3.2 MB/s eta 0:00:10\n",
      "   --------- ------------------------------ 10.2/41.0 MB 3.2 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 11.0/41.0 MB 3.2 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 11.5/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ----------- ---------------------------- 12.1/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------ --------------------------- 12.6/41.0 MB 3.1 MB/s eta 0:00:10\n",
      "   ------------- -------------------------- 13.4/41.0 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 13.9/41.0 MB 3.1 MB/s eta 0:00:09\n",
      "   ------------- -------------------------- 14.2/41.0 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 14.4/41.0 MB 3.0 MB/s eta 0:00:09\n",
      "   -------------- ------------------------- 15.2/41.0 MB 3.0 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 15.7/41.0 MB 2.9 MB/s eta 0:00:09\n",
      "   --------------- ------------------------ 16.0/41.0 MB 2.9 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 16.5/41.0 MB 2.9 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.0/41.0 MB 2.8 MB/s eta 0:00:09\n",
      "   ---------------- ----------------------- 17.3/41.0 MB 2.8 MB/s eta 0:00:09\n",
      "   ----------------- ---------------------- 18.1/41.0 MB 2.8 MB/s eta 0:00:09\n",
      "   ------------------ --------------------- 18.6/41.0 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------ --------------------- 19.1/41.0 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 19.7/41.0 MB 2.8 MB/s eta 0:00:08\n",
      "   ------------------- -------------------- 20.2/41.0 MB 2.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 21.0/41.0 MB 2.8 MB/s eta 0:00:08\n",
      "   -------------------- ------------------- 21.2/41.0 MB 2.8 MB/s eta 0:00:08\n",
      "   --------------------- ------------------ 21.8/41.0 MB 2.8 MB/s eta 0:00:07\n",
      "   --------------------- ------------------ 22.3/41.0 MB 2.8 MB/s eta 0:00:07\n",
      "   ---------------------- ----------------- 22.8/41.0 MB 2.8 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 23.6/41.0 MB 2.8 MB/s eta 0:00:07\n",
      "   ----------------------- ---------------- 24.4/41.0 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------ --------------- 24.9/41.0 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 25.7/41.0 MB 2.8 MB/s eta 0:00:06\n",
      "   ------------------------- -------------- 26.5/41.0 MB 2.8 MB/s eta 0:00:06\n",
      "   -------------------------- ------------- 27.3/41.0 MB 2.9 MB/s eta 0:00:05\n",
      "   --------------------------- ------------ 28.6/41.0 MB 2.9 MB/s eta 0:00:05\n",
      "   ---------------------------- ----------- 29.6/41.0 MB 3.0 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 30.7/41.0 MB 3.0 MB/s eta 0:00:04\n",
      "   ------------------------------ --------- 31.7/41.0 MB 3.1 MB/s eta 0:00:04\n",
      "   ------------------------------- -------- 32.8/41.0 MB 3.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 33.6/41.0 MB 3.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 34.6/41.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ---------------------------------- ----- 35.4/41.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 36.2/41.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.0/41.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 37.7/41.0 MB 3.2 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 38.8/41.0 MB 3.2 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 39.6/41.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------  40.4/41.0 MB 3.3 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 41.0/41.0 MB 3.3 MB/s eta 0:00:00\n",
      "Downloading threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, scipy, joblib, scikit-learn\n",
      "\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   ---------- ----------------------------- 1/4 [scipy]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   -------------------- ------------------- 2/4 [joblib]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ------------------------------ --------- 3/4 [scikit-learn]\n",
      "   ---------------------------------------- 4/4 [scikit-learn]\n",
      "\n",
      "Successfully installed joblib-1.5.1 scikit-learn-1.7.0 scipy-1.15.3 threadpoolctl-3.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install pandas \n",
    "!pip3 install h5py \n",
    "!pip3 install tqdm \n",
    "!pip3 install torchinfo\n",
    "!pip3 install matplotlib \n",
    "!pip3 install seaborn \n",
    "!pip3 install scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e4d70220",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e3878547",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import h5py\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.cuda.amp import GradScaler, autocast\n",
    "from torchinfo import summary\n",
    "import torch.utils.checkpoint as checkpoint\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from tqdm import tqdm, trange\n",
    "import seaborn as sns\n",
    "import gc\n",
    "import time\n",
    "import warnings\n",
    "from collections import deque\n",
    "import psutil\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "58392c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_channels=2\n",
    "batch_size = 512      # or 128, 256, 512, etc.\n",
    "frame_size = 1024    # fixed by dataset (1024 I/Q samples per frame)\n",
    "n_labels = 19\n",
    "nf_train = int(batch_size * 0.7)\n",
    "nf_valid = int(batch_size * 0.2)\n",
    "nf_test  = batch_size - nf_train - nf_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fbb33d57",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dataset_split(data,\n",
    "                  modulations_classes,\n",
    "                  modulations,\n",
    "                  snrs,\n",
    "                  target_modulations,\n",
    "                  mode,\n",
    "                  target_snrs,\n",
    "                  train_proportion=0.7, # training 70 %\n",
    "                  valid_proportion=0.2, # validation 20 %\n",
    "                  test_proportion=0.1, # testing 10 % \n",
    "                  seed=48):\n",
    "    np.random.seed(seed)\n",
    "    X_output = []\n",
    "    Y_output = []\n",
    "    Z_output = []\n",
    "\n",
    "    target_modulation_indices = [modulations_classes.index(modu) for modu in target_modulations]\n",
    "\n",
    "    for modu in target_modulation_indices:\n",
    "        for snr in target_snrs:\n",
    "            snr_modu_indices = np.where((modulations == modu) & (snrs == snr))[0]\n",
    "\n",
    "            np.random.shuffle(snr_modu_indices)\n",
    "            num_samples = len(snr_modu_indices)\n",
    "            train_end = int(train_proportion * num_samples)\n",
    "            valid_end = int((train_proportion + valid_proportion) * num_samples)\n",
    "\n",
    "            if mode == 'train':\n",
    "                indices = snr_modu_indices[:train_end]\n",
    "            elif mode == 'valid':\n",
    "                indices = snr_modu_indices[train_end:valid_end]\n",
    "            elif mode == 'test':\n",
    "                indices = snr_modu_indices[valid_end:]\n",
    "            else:\n",
    "                raise ValueError(f'unknown mode: {mode}. Valid modes are train, valid and test')\n",
    "\n",
    "            X_output.append(data[np.sort(indices)])\n",
    "            Y_output.append(modulations[np.sort(indices)])\n",
    "            Z_output.append(snrs[np.sort(indices)])\n",
    "\n",
    "    X_array = np.vstack(X_output)\n",
    "    Y_array = np.concatenate(Y_output)\n",
    "    Z_array = np.concatenate(Z_output)\n",
    "    for index, value in enumerate(np.unique(np.copy(Y_array))):\n",
    "        Y_array[Y_array == value] = index\n",
    "    return X_array, Y_array, Z_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dc83ea6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RadioML18Dataset(Dataset):\n",
    "    def __init__(self, mode: str, seed=48,):\n",
    "        super(RadioML18Dataset, self).__init__()\n",
    "\n",
    "        # load data\n",
    "        hdf5_file = h5py.File(\"C:\\\\workarea\\\\CNN model\\\\dataset\\\\radioml2018\\\\versions\\\\2\\\\GOLD_XYZ_OSC.0001_1024.hdf5\", 'r') #Escaped backslashes \n",
    "        self.modulation_classes = json.load(open(\"C:\\\\workarea\\\\CNN model\\\\dataset\\\\radioml2018\\\\versions\\\\2\\\\classes-fixed.json\", 'r'))\n",
    "        self.X = hdf5_file['X']\n",
    "        self.Y = np.argmax(hdf5_file['Y'], axis=1)\n",
    "        self.Z = hdf5_file['Z'][:, 0]\n",
    "\n",
    "        train_proportion=(14*26*nf_train)/self.X.shape[0]\n",
    "        valid_proportion=(14*26*nf_valid)/self.X.shape[0]\n",
    "        test_proportion=(14*26*nf_test)/self.X.shape[0]\n",
    "\n",
    "        \"\"\"target_modulations =['OOK', '4ASK', 'BPSK', 'QPSK', '8PSK',\n",
    "        '16QAM', 'AM-SSB-SC', 'AM-DSB-SC', 'FM', 'GMSK','OQPSK']target\n",
    "        modulation class and snr\"\"\"\n",
    "\n",
    "        # in this line i could change it the target modulation\n",
    "        self.target_modulations = ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK','16APSK', '32APSK', '64APSK', '16QAM', '32QAM','64QAM']\n",
    "\n",
    "        self.target_snrs = np.unique(self.Z)\n",
    "\n",
    "        self.X_data, self.Y_data, self.Z_data = dataset_split(\n",
    "                                                                  data = self.X,\n",
    "                                                                  modulations_classes = self.modulation_classes,\n",
    "                                                                  modulations = self.Y,\n",
    "                                                                  snrs = self.Z,\n",
    "                                                                  mode = mode,\n",
    "                                                                  train_proportion = train_proportion,\n",
    "                                                                  valid_proportion = valid_proportion,\n",
    "                                                                  test_proportion = test_proportion,\n",
    "                                                                  target_modulations = self.target_modulations,\n",
    "                                                                  target_snrs  = self.target_snrs,\n",
    "                                                                  seed=48\n",
    "                                                                 )\n",
    "\n",
    "        # *** CRITICAL FIX: Apply I/Q swap correction for AMC compatibility ***\n",
    "        print(f\"ðŸ”§ Applying I/Q swap fix to {mode} dataset...\")\n",
    "        X_corrected = np.zeros_like(self.X_data)\n",
    "        X_corrected[:, :, 0] = self.X_data[:, :, 1]  # I = original Q\n",
    "        X_corrected[:, :, 1] = self.X_data[:, :, 0]  # Q = original I\n",
    "        self.X_data = X_corrected\n",
    "        print(f\"âœ… I/Q channels corrected for real-world compatibility\")\n",
    "\n",
    "        # store statistic of whole dataset (unchanged)\n",
    "        self.num_data = self.X_data.shape[0]\n",
    "        self.num_lbl = len(self.target_modulations)\n",
    "        self.num_snr = self.target_snrs.shape[0]\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X_data.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x,y,z = self.X_data[idx], self.Y_data[idx], self.Z_data[idx]\n",
    "        x,y,z = torch.Tensor(x).transpose(0, 1) , y , z\n",
    "        return x,y,z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "bea848e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Applying I/Q swap fix to test dataset...\n",
      "âœ… I/Q channels corrected for real-world compatibility\n",
      "1393392\n",
      "14\n",
      "26\n",
      "1024\n"
     ]
    }
   ],
   "source": [
    "ds = RadioML18Dataset(mode='test')\n",
    "data_len = ds.num_data\n",
    "n_labels=ds.num_lbl\n",
    "n_snrs = ds.num_snr\n",
    "frame_size=ds.X.shape[1]\n",
    "\n",
    "print(data_len)\n",
    "print(n_labels)\n",
    "print(n_snrs)\n",
    "print(frame_size)\n",
    "del ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e3f939c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Applying I/Q swap fix to train dataset...\n",
      "âœ… I/Q channels corrected for real-world compatibility\n",
      "All Modulation Classes: ['OOK', '4ASK', '8ASK', 'BPSK', 'QPSK', '8PSK', '16PSK', '32PSK', '16APSK', '32APSK', '64APSK', '128APSK', '16QAM', '32QAM', '64QAM', '128QAM', '256QAM', 'AM-SSB-WC', 'AM-SSB-SC', 'AM-DSB-WC', 'AM-DSB-SC', 'FM', 'GMSK', 'OQPSK']\n"
     ]
    }
   ],
   "source": [
    "dataset = RadioML18Dataset(mode='train')\n",
    "\n",
    "# Print all modulation classes\n",
    "print(\"All Modulation Classes:\", dataset.modulation_classes)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "bf3781ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”§ Applying I/Q swap fix to train dataset...\n",
      "âœ… I/Q channels corrected for real-world compatibility\n",
      "ðŸ”§ Applying I/Q swap fix to valid dataset...\n",
      "âœ… I/Q channels corrected for real-world compatibility\n",
      "ðŸ”§ Applying I/Q swap fix to test dataset...\n",
      "âœ… I/Q channels corrected for real-world compatibility\n",
      "Execution time : 35.5287549495697 second\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "st = time.time()\n",
    "train_dl = DataLoader(dataset=RadioML18Dataset(mode='train'),batch_size = 64, shuffle = True, drop_last = True)\n",
    "valid_dl = DataLoader(dataset=RadioML18Dataset(mode='valid'),batch_size = 128, shuffle = True, drop_last = False)\n",
    "test_dl = DataLoader(dataset=RadioML18Dataset(mode='test'), batch_size = 128, shuffle = True, drop_last = False)\n",
    "et = time.time()\n",
    "elapsed_time = et - st\n",
    "print(f'Execution time : {elapsed_time} second')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "d471ebb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedCNN_Block(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, dropout_rate=0.3):\n",
    "        super().__init__()\n",
    "        self.block = nn.Sequential(\n",
    "            nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm1d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool1d(kernel_size=2),\n",
    "            nn.Dropout(dropout_rate)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.block(x)\n",
    "\n",
    "class ImprovedCNN_NET(nn.Module):\n",
    "    def __init__(self, n_labels, dropout_rate=0.4):\n",
    "        super().__init__()\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            ImprovedCNN_Block(2, 32, dropout_rate=0.2),\n",
    "            ImprovedCNN_Block(32, 64, dropout_rate=0.3),\n",
    "            ImprovedCNN_Block(64, 128, dropout_rate=0.3),\n",
    "            nn.AdaptiveAvgPool1d(16)\n",
    "        )\n",
    "\n",
    "        # Enhanced classifier for 19-class problem\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(128 * 16, 512),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.75),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(dropout_rate * 0.5),\n",
    "\n",
    "            nn.Linear(128, n_labels)  # This will be 19 for full dataset\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        return self.classifier(x)\n",
    "\n",
    "def create_improved_model(n_labels, dropout_rate=0.4):\n",
    "    \"\"\"Create improved CNN model - now works for any number of classes\"\"\"\n",
    "    return ImprovedCNN_NET(n_labels, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "972e41cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EarlyStopping:\n",
    "    def __init__(self, patience=15, min_delta=0.001, restore_best_weights=True):\n",
    "        self.patience = patience\n",
    "        self.min_delta = min_delta\n",
    "        self.restore_best_weights = restore_best_weights\n",
    "        self.best_loss = float('inf')\n",
    "        self.counter = 0\n",
    "        self.best_weights = None\n",
    "\n",
    "    def __call__(self, val_loss, model):\n",
    "        if val_loss < self.best_loss - self.min_delta:\n",
    "            self.best_loss = val_loss\n",
    "            self.counter = 0\n",
    "            if self.restore_best_weights:\n",
    "                self.best_weights = model.state_dict().copy()\n",
    "        else:\n",
    "            self.counter += 1\n",
    "\n",
    "        if self.counter >= self.patience:\n",
    "            if self.restore_best_weights and self.best_weights is not None:\n",
    "                model.load_state_dict(self.best_weights)\n",
    "            return True\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "75a4e4bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "ImprovedCNN_NET                          [1, 14]                   --\n",
       "â”œâ”€Sequential: 1-1                        [1, 128, 16]              --\n",
       "â”‚    â””â”€ImprovedCNN_Block: 2-1            [1, 32, 512]              --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-1              [1, 32, 512]              3,456\n",
       "â”‚    â””â”€ImprovedCNN_Block: 2-2            [1, 64, 256]              --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-2              [1, 64, 256]              18,816\n",
       "â”‚    â””â”€ImprovedCNN_Block: 2-3            [1, 128, 128]             --\n",
       "â”‚    â”‚    â””â”€Sequential: 3-3              [1, 128, 128]             74,496\n",
       "â”‚    â””â”€AdaptiveAvgPool1d: 2-4            [1, 128, 16]              --\n",
       "â”œâ”€Sequential: 1-2                        [1, 14]                   --\n",
       "â”‚    â””â”€Flatten: 2-5                      [1, 2048]                 --\n",
       "â”‚    â””â”€Linear: 2-6                       [1, 512]                  1,049,088\n",
       "â”‚    â””â”€BatchNorm1d: 2-7                  [1, 512]                  1,024\n",
       "â”‚    â””â”€ReLU: 2-8                         [1, 512]                  --\n",
       "â”‚    â””â”€Dropout: 2-9                      [1, 512]                  --\n",
       "â”‚    â””â”€Linear: 2-10                      [1, 256]                  131,328\n",
       "â”‚    â””â”€BatchNorm1d: 2-11                 [1, 256]                  512\n",
       "â”‚    â””â”€ReLU: 2-12                        [1, 256]                  --\n",
       "â”‚    â””â”€Dropout: 2-13                     [1, 256]                  --\n",
       "â”‚    â””â”€Linear: 2-14                      [1, 128]                  32,896\n",
       "â”‚    â””â”€ReLU: 2-15                        [1, 128]                  --\n",
       "â”‚    â””â”€Dropout: 2-16                     [1, 128]                  --\n",
       "â”‚    â””â”€Linear: 2-17                      [1, 14]                   1,806\n",
       "==========================================================================================\n",
       "Total params: 1,313,422\n",
       "Trainable params: 1,313,422\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (Units.MEGABYTES): 33.07\n",
       "==========================================================================================\n",
       "Input size (MB): 0.01\n",
       "Forward/backward pass size (MB): 3.16\n",
       "Params size (MB): 5.25\n",
       "Estimated Total Size (MB): 8.42\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model =ImprovedCNN_NET(n_labels).to('cuda')\n",
    "\n",
    "# Safer: create explicit dummy input\n",
    "dummy_input = torch.randn(1, n_channels, frame_size).to('cuda')\n",
    "\n",
    "summary(model, input_data=dummy_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3c3ce911",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 1. IMPROVED TRAINING FUNCTION\n",
    "# ============================================================================\n",
    "\n",
    "def train_model(model, train_dl=train_dl, valid_dl=valid_dl, verbose=True, device='cuda', num_epoch=200,\n",
    "                accumulation_steps=1, use_amp=True, memory_cleanup_freq=10):\n",
    "    \"\"\"\n",
    "    GPU memory-optimized training function for Google Colab Pro\n",
    "\n",
    "    Args:\n",
    "        model: Neural network model\n",
    "        train_dl: Training dataloader\n",
    "        valid_dl: Validation dataloader\n",
    "        verbose: Print training progress\n",
    "        device: Device to train on\n",
    "        num_epoch: Maximum number of epochs\n",
    "        accumulation_steps: Gradient accumulation steps (effective batch size = batch_size * accumulation_steps)\n",
    "        use_amp: Use Automatic Mixed Precision (saves ~40-50% GPU memory)\n",
    "        memory_cleanup_freq: How often to clean GPU memory (every N epochs)\n",
    "    \"\"\"\n",
    "\n",
    "    # GPU memory check and optimization\n",
    "    if device == 'cuda':\n",
    "        print(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "        print(f\"Initial GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB / {torch.cuda.memory_reserved()/1024**3:.2f}GB\")\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    # Use deque for memory-efficient history storage (only keep recent values)\n",
    "    history_size = min(num_epoch, 1000)  # Limit history to prevent memory issues\n",
    "    train_loss_history = deque(maxlen=history_size)\n",
    "    train_acc_history = deque(maxlen=history_size)\n",
    "    val_loss_history = deque(maxlen=history_size)\n",
    "    val_acc_history = deque(maxlen=history_size)\n",
    "\n",
    "    # Improved optimizer with memory-efficient settings\n",
    "    lr = 1e-4\n",
    "    optimizer = optim.AdamW(\n",
    "        model.parameters(),\n",
    "        lr=lr,\n",
    "        weight_decay=1e-4,\n",
    "        eps=1e-8,  # Slightly larger eps for numerical stability in mixed precision\n",
    "        amsgrad=False  # Disable amsgrad to save memory\n",
    "    )\n",
    "\n",
    "    # Learning rate scheduler\n",
    "    lr_scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer,\n",
    "        mode='min',\n",
    "        factor=0.5,\n",
    "        patience=10,\n",
    "        #verbose=True,\n",
    "        min_lr=1e-6\n",
    "    )\n",
    "\n",
    "    # setup the new path for the model \n",
    "    output_dir = r\"C:\\workarea\\CNN model\\model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Label smoothing for better generalization\n",
    "    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)\n",
    "\n",
    "    # Mixed precision training setup\n",
    "    scaler = GradScaler() if use_amp else None\n",
    "\n",
    "    # Early stopping variables\n",
    "    best_val_loss = float('inf')\n",
    "    best_val_acc = 0.0\n",
    "    patience = 20\n",
    "    patience_counter = 0\n",
    "    best_model_path = os.path.join(output_dir, 'best_model.pth')  # Save to disk instead of memory\n",
    "\n",
    "    actual_epochs = 0\n",
    "\n",
    "    try:\n",
    "        for epoch in trange(num_epoch, desc='Training'):\n",
    "            actual_epochs = epoch + 1\n",
    "\n",
    "            # Memory cleanup every N epochs\n",
    "            if epoch % memory_cleanup_freq == 0 and epoch > 0:\n",
    "                gc.collect()\n",
    "                if device == 'cuda':\n",
    "                    torch.cuda.empty_cache()\n",
    "                    if verbose:\n",
    "                        tqdm.write(f\"GPU memory after cleanup: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "            # ----- Training Phase -----\n",
    "            model.train()\n",
    "            total_train_loss, total_train_correct, total_train_samples = 0.0, 0, 0\n",
    "\n",
    "            # Reset gradients outside the loop\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            for batch_idx, (x, y, _) in enumerate(train_dl):\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "                # Gradient accumulation context\n",
    "                # FIXED: Correctly use autocast with device_type and enabled\n",
    "                with autocast(enabled=use_amp):\n",
    "                    # Optional: Add noise augmentation (but less frequently to save memory)\n",
    "                    if torch.rand(1).item() < 0.2:  # Reduced from 30% to 20%\n",
    "                        noise = torch.randn_like(x) * 0.05\n",
    "                        x = x + noise\n",
    "\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "\n",
    "                    # Scale loss for gradient accumulation\n",
    "                    loss = loss / accumulation_steps\n",
    "\n",
    "                # Backward pass with mixed precision\n",
    "                if use_amp:\n",
    "                    scaler.scale(loss).backward()\n",
    "                else:\n",
    "                    loss.backward()\n",
    "\n",
    "                # Update weights every accumulation_steps\n",
    "                if (batch_idx + 1) % accumulation_steps == 0:\n",
    "                    if use_amp:\n",
    "                        scaler.unscale_(optimizer)\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        scaler.step(optimizer)\n",
    "                        scaler.update()\n",
    "                    else:\n",
    "                        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                        optimizer.step()\n",
    "\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                # Accumulate statistics (scale back the loss)\n",
    "                with torch.no_grad():\n",
    "                    total_train_loss += (loss.item() * accumulation_steps) * x.size(0)\n",
    "                    total_train_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                    total_train_samples += x.size(0)\n",
    "\n",
    "                # Clear intermediate tensors\n",
    "                del x, y, logits, loss\n",
    "\n",
    "            # Handle remaining gradients\n",
    "            if (len(train_dl) % accumulation_steps) != 0:\n",
    "                if use_amp:\n",
    "                    scaler.unscale_(optimizer)\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    scaler.step(optimizer)\n",
    "                    scaler.update()\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                    optimizer.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            epoch_train_loss = total_train_loss / total_train_samples\n",
    "            epoch_train_acc = total_train_correct / total_train_samples\n",
    "\n",
    "            train_loss_history.append(epoch_train_loss)\n",
    "            train_acc_history.append(epoch_train_acc)\n",
    "\n",
    "            # ----- Validation Phase -----\n",
    "            model.eval()\n",
    "            total_val_loss, total_val_correct, total_val_samples = 0.0, 0, 0\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for x, y, _ in valid_dl:\n",
    "                    x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "\n",
    "                    # FIXED: Correctly use autocast with device_type and enabled\n",
    "                    with autocast(enabled=use_amp):\n",
    "                        logits = model(x)\n",
    "                        loss = criterion(logits, y)\n",
    "\n",
    "                    total_val_loss += loss.item() * x.size(0)\n",
    "                    total_val_correct += (logits.argmax(dim=1) == y).sum().item()\n",
    "                    total_val_samples += x.size(0)\n",
    "\n",
    "                    # Clear tensors immediately\n",
    "                    del x, y, logits, loss\n",
    "\n",
    "            epoch_val_loss = total_val_loss / total_val_samples\n",
    "            epoch_val_acc = total_val_correct / total_val_samples\n",
    "\n",
    "            val_loss_history.append(epoch_val_loss)\n",
    "            val_acc_history.append(epoch_val_acc)\n",
    "\n",
    "            # Update learning rate\n",
    "            lr_scheduler.step(epoch_val_loss)\n",
    "\n",
    "            # Early stopping logic with disk-based model saving\n",
    "            if epoch_val_loss < best_val_loss - 0.001:\n",
    "                best_val_loss = epoch_val_loss\n",
    "                best_val_acc = epoch_val_acc\n",
    "                # Save best model to disk instead of keeping in memory\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                patience_counter = 0\n",
    "            else:\n",
    "                patience_counter += 1\n",
    "\n",
    "            # Check if we should stop early\n",
    "            if patience_counter >= patience:\n",
    "                if verbose:\n",
    "                    tqdm.write(f\"\\nEarly stopping triggered at epoch {epoch+1}\")\n",
    "                    tqdm.write(f\"Best validation loss: {best_val_loss:.4f}, Best validation acc: {best_val_acc:.4f}\")\n",
    "\n",
    "                # Load best model from disk\n",
    "                try:\n",
    "                    model.load_state_dict(torch.load(best_model_path, map_location=device))\n",
    "                except:\n",
    "                    tqdm.write(\"Warning: Could not load best model state\")\n",
    "                break\n",
    "\n",
    "            # Memory-conscious verbose output\n",
    "            if verbose:\n",
    "                show_progress = (\n",
    "                    (epoch + 1) % 10 == 0 or\n",
    "                    epoch < 10 or\n",
    "                    epoch_val_acc > best_val_acc or\n",
    "                    patience_counter == 0\n",
    "                )\n",
    "\n",
    "                if show_progress:\n",
    "                    current_lr = optimizer.param_groups[0]['lr']\n",
    "                    tqdm.write(f\"Epoch {epoch+1:03d} | Train Loss: {epoch_train_loss:.4f}, Acc: {epoch_train_acc:.4f}\")\n",
    "                    tqdm.write(f\"            | Val   Loss: {epoch_val_loss:.4f}, Acc: {epoch_val_acc:.4f}\")\n",
    "                    tqdm.write(f\"            | LR: {current_lr:.6f}, Patience: {patience_counter}/{patience}\")\n",
    "\n",
    "                    if device == 'cuda':\n",
    "                        tqdm.write(f\"            | GPU Memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "                    if epoch_val_acc > best_val_acc:\n",
    "                        tqdm.write(f\"            | *** New best validation accuracy! ***\")\n",
    "\n",
    "            # Force memory cleanup every few epochs\n",
    "            if epoch % 5 == 0:\n",
    "                gc.collect()\n",
    "\n",
    "    except RuntimeError as e:\n",
    "        if \"out of memory\" in str(e):\n",
    "            print(f\"\\nGPU out of memory error at epoch {epoch+1}\")\n",
    "            print(\"Try reducing batch size, enabling gradient accumulation, or using mixed precision\")\n",
    "            print(f\"Current GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "            # Emergency cleanup\n",
    "            gc.collect()\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "        raise e\n",
    "\n",
    "    finally:\n",
    "        # Final cleanup\n",
    "        gc.collect()\n",
    "        if device == 'cuda':\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Convert deque to lists for compatibility\n",
    "    train_history = {\n",
    "        'train_loss': list(train_loss_history),\n",
    "        'train_acc': list(train_acc_history),\n",
    "        'val_loss': list(val_loss_history),\n",
    "        'val_acc': list(val_acc_history),\n",
    "        'best_val_loss': best_val_loss,\n",
    "        'best_val_acc': best_val_acc,\n",
    "        'epochs_trained': actual_epochs\n",
    "    }\n",
    "\n",
    "    if verbose:\n",
    "        print(f\"\\nTraining completed!\")\n",
    "        print(f\"Epochs trained: {actual_epochs}\")\n",
    "        print(f\"Final validation accuracy: {val_acc_history[-1]:.4f}\")\n",
    "        print(f\"Best validation accuracy: {best_val_acc:.4f}\")\n",
    "        if device == 'cuda':\n",
    "            print(f\"Final GPU memory: {torch.cuda.memory_allocated()/1024**3:.2f}GB\")\n",
    "\n",
    "    return model, train_history\n",
    "\n",
    "\n",
    "def get_memory_usage():\n",
    "    \"\"\"Utility function to check current GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        allocated = torch.cuda.memory_allocated() / 1024**3\n",
    "        reserved = torch.cuda.memory_reserved() / 1024**3\n",
    "        print(f\"GPU Memory - Allocated: {allocated:.2f}GB, Reserved: {reserved:.2f}GB\")\n",
    "        return allocated, reserved\n",
    "    return 0, 0\n",
    "\n",
    "\n",
    "def optimize_dataloader_for_gpu(dataset, batch_size=512, num_workers=2):\n",
    "    \"\"\"\n",
    "    Create memory-optimized dataloader for Colab Pro\n",
    "    \"\"\"\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=num_workers,  # Reduced for Colab\n",
    "        pin_memory=True,  # Faster GPU transfer\n",
    "        persistent_workers=True if num_workers > 0 else False,\n",
    "        prefetch_factor=2,  # Reduced prefetch\n",
    "        drop_last=True  # Ensures consistent batch sizes\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "097acd93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 2. IMPROVED MODEL ARCHITECTURE\n",
    "# ============================================================================\n",
    "\n",
    "def create_improved_model(n_labels, dropout_rate=0.4):\n",
    "    \"\"\"\n",
    "    Create an improved CNN model with better regularization\n",
    "    \"\"\"\n",
    "    class ImprovedCNN_Block(nn.Module):\n",
    "        def __init__(self, in_channels, out_channels, dropout_rate=0.3):\n",
    "            super().__init__()\n",
    "            self.block = nn.Sequential(\n",
    "                nn.Conv1d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Conv1d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "                nn.BatchNorm1d(out_channels),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.MaxPool1d(kernel_size=2),\n",
    "                nn.Dropout(dropout_rate)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            return self.block(x)\n",
    "\n",
    "    class ImprovedCNN_NET(nn.Module):\n",
    "        def __init__(self, n_labels, dropout_rate=0.4):\n",
    "            super().__init__()\n",
    "\n",
    "            self.backbone = nn.Sequential(\n",
    "                ImprovedCNN_Block(2, 32, dropout_rate=0.2),\n",
    "                ImprovedCNN_Block(32, 64, dropout_rate=0.3),\n",
    "                ImprovedCNN_Block(64, 128, dropout_rate=0.3),\n",
    "                nn.AdaptiveAvgPool1d(16)\n",
    "            )\n",
    "\n",
    "            self.classifier = nn.Sequential(\n",
    "                nn.Flatten(),\n",
    "                nn.Linear(128 * 16, 512),\n",
    "                nn.BatchNorm1d(512),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_rate),\n",
    "\n",
    "                nn.Linear(512, 256),\n",
    "                nn.BatchNorm1d(256),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_rate * 0.75),\n",
    "\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(inplace=True),\n",
    "                nn.Dropout(dropout_rate * 0.5),\n",
    "\n",
    "                nn.Linear(128, n_labels)\n",
    "            )\n",
    "\n",
    "        def forward(self, x):\n",
    "            x = self.backbone(x)\n",
    "            return self.classifier(x)\n",
    "\n",
    "    return ImprovedCNN_NET(n_labels, dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d1f221f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ============================================================================\n",
    "# 3. IMPROVED TESTING AND ANALYSIS FUNCTIONS\n",
    "# ============================================================================\n",
    "\n",
    "def test_model_with_improved_plots(model, device='cuda'):\n",
    "    \"\"\"\n",
    "    Enhanced testing function with better memory management and error handling\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    Y_pred_ = []  # Predictions\n",
    "    Y_true_ = []  # Ground truth\n",
    "    Z_snr_ = []   # SNR values\n",
    "\n",
    "    # FIXED: Use actual test dataset instead of train\n",
    "    test_dataset = RadioML18Dataset(mode='test')\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=128, shuffle=False, drop_last=False)\n",
    "\n",
    "    target_classes = test_dataset.target_modulations\n",
    "    target_snrs = test_dataset.target_snrs\n",
    "    modulation_classes = test_dataset.modulation_classes\n",
    "\n",
    "    # Add debug\n",
    "    print(f\"Target modulations: {target_classes}\")\n",
    "    print(f\"Target SNRs: {target_snrs}\")\n",
    "    print(f\"Test dataset size: {len(test_dataset)}\")\n",
    "\n",
    "    # Initialize accuracy stats DataFrame\n",
    "    accuracy_stats = pd.DataFrame(\n",
    "        0.0,\n",
    "        index=target_classes,\n",
    "        columns=target_snrs.astype('str'))\n",
    "\n",
    "    # Get predictions with tqdm progress bar\n",
    "    test_progress = tqdm(test_loader, desc=\"Testing model\", leave=True)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, (x, y, z) in enumerate(test_progress):\n",
    "            # Move tensors to specified device\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            z = z.to(device)\n",
    "\n",
    "            # Get model predictions on device\n",
    "            logits = model(x)\n",
    "            y_pred = torch.argmax(logits, dim=-1)\n",
    "\n",
    "            # Store results\n",
    "            Y_pred_.append(y_pred.cpu())  # Move back to CPU for storage\n",
    "            Y_true_.append(y.cpu())\n",
    "            Z_snr_.append(z.cpu())\n",
    "\n",
    "            # Update progress bar\n",
    "            if batch_idx % 10 == 0:\n",
    "                current_acc = (y_pred == y).float().mean().item()\n",
    "                test_progress.set_postfix({\"batch_acc\": f\"{current_acc:.3f}\"})\n",
    "\n",
    "            # Free up memory\n",
    "            del x, y, z, logits, y_pred\n",
    "            if device == 'cuda':\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    # Convert to numpy for easier processing\n",
    "    Y_pred = torch.cat(Y_pred_).numpy()\n",
    "    Y_true = torch.cat(Y_true_).numpy()\n",
    "    Z_snr = torch.cat(Z_snr_).numpy()\n",
    "\n",
    "    # Clear lists to free memory\n",
    "    del Y_pred_, Y_true_, Z_snr_\n",
    "\n",
    "    # Calculate overall accuracy\n",
    "    correct_preds = (Y_pred == Y_true).sum()\n",
    "    total_samples = len(Y_true)\n",
    "    total_accuracy = round(correct_preds * 100 / total_samples, 2)\n",
    "    print(f'Overall test accuracy: {total_accuracy}%')\n",
    "\n",
    "    # Count samples for each modulation type\n",
    "    mod_counts = {}\n",
    "    for mod_idx, mod_name in enumerate(target_classes):\n",
    "        count = np.sum(Y_true == mod_idx)\n",
    "        mod_counts[mod_name] = count\n",
    "        print(f\"Modulation {mod_name}: {count} test samples\")\n",
    "\n",
    "    # Calculate accuracy per modulation and SNR with progress bar\n",
    "    mod_snr_progress = tqdm(list(enumerate(target_classes)),\n",
    "                           desc=\"Calculating per-modulation accuracies\",\n",
    "                           leave=True)\n",
    "\n",
    "    for mod_idx, mod_name in mod_snr_progress:\n",
    "        mod_snr_progress.set_postfix({\"modulation\": mod_name})\n",
    "        for snr_idx, snr in enumerate(target_snrs):\n",
    "            snr_str = str(snr)\n",
    "\n",
    "            mask = (Y_true == mod_idx) & (Z_snr == snr)\n",
    "            total_samples = mask.sum()\n",
    "            if total_samples > 0:\n",
    "                correct_samples = ((Y_pred == Y_true) & mask).sum()\n",
    "                accuracy = (correct_samples * 100 / total_samples)\n",
    "                accuracy_stats.loc[mod_name, snr_str] = round(accuracy, 2)\n",
    "            else:\n",
    "                accuracy_stats.loc[mod_name, snr_str] = np.nan\n",
    "                print(f\"Warning: no samples for {mod_name} at SNR = {snr}\")\n",
    "\n",
    "    return accuracy_stats, mod_counts, Y_true, Y_pred, target_classes\n",
    "\n",
    "def plot_confusion_matrix(Y_true, Y_pred, target_classes, save_name='confusion_matrix'):\n",
    "    \"\"\"\n",
    "    Plot confusion matrix for better understanding of misclassifications\n",
    "    \"\"\"\n",
    "    cm = confusion_matrix(Y_true, Y_pred)\n",
    "\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',\n",
    "                xticklabels=target_classes,\n",
    "                yticklabels=target_classes)\n",
    "    plt.title('Confusion Matrix - Signal Modulation Classification')\n",
    "    plt.xlabel('Predicted Modulation')\n",
    "    plt.ylabel('True Modulation')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_name}.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print classification report\n",
    "    print(\"\\nDetailed Classification Report:\")\n",
    "    print(classification_report(Y_true, Y_pred, target_names=target_classes))\n",
    "\n",
    "def plot_improved_test_accuracy(model, device='cuda', save_prefix='model'):\n",
    "    \"\"\"\n",
    "    Enhanced plotting function with confusion matrix and better analysis\n",
    "    \"\"\"\n",
    "    accuracy_df, mod_counts, Y_true, Y_pred, target_classes = test_model_with_improved_plots(model, device)\n",
    "\n",
    "    # 1. Plot confusion matrix first\n",
    "    plot_confusion_matrix(Y_true, Y_pred, target_classes, f'{save_prefix}_confusion_matrix')\n",
    "\n",
    "    # 2. Overall accuracy vs SNR plot\n",
    "    plt.figure(figsize=(14, 8))\n",
    "\n",
    "    accuracy_long = accuracy_df.reset_index().melt(\n",
    "        id_vars=['index'],\n",
    "        var_name='SNR',\n",
    "        value_name='Accuracy'\n",
    "    )\n",
    "    accuracy_long.columns = ['Modulation', 'SNR', 'Accuracy']\n",
    "\n",
    "    # Convert SNR to numeric for proper ordering\n",
    "    accuracy_long['SNR_numeric'] = accuracy_long['SNR'].astype(int)\n",
    "    accuracy_long = accuracy_long.sort_values('SNR_numeric')\n",
    "\n",
    "    sns.lineplot(\n",
    "        data=accuracy_long,\n",
    "        x='SNR_numeric',\n",
    "        y='Accuracy',\n",
    "        hue='Modulation',\n",
    "        marker='o',\n",
    "        markersize=8,\n",
    "        linewidth=2\n",
    "    )\n",
    "\n",
    "    # Highlight PSK modulations\n",
    "    psk_mods = [mod for mod in accuracy_df.index if 'PSK' in mod]\n",
    "    if psk_mods:\n",
    "        print(f\"Highlighting PSK modulations: {psk_mods}\")\n",
    "        for mod in psk_mods:\n",
    "            mod_data = accuracy_long[accuracy_long['Modulation'] == mod]\n",
    "            if not mod_data.empty:\n",
    "                plt.plot(mod_data['SNR_numeric'], mod_data['Accuracy'],\n",
    "                         linewidth=4,\n",
    "                         linestyle='--',\n",
    "                         marker='*',\n",
    "                         markersize=12,\n",
    "                         alpha=0.8)\n",
    "\n",
    "    plt.title('Classification Accuracy vs SNR for Different Modulation Types', fontsize=16)\n",
    "    plt.xlabel('Signal-to-Noise Ratio (dB)', fontsize=14)\n",
    "    plt.ylabel('Accuracy (%)', fontsize=14)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_prefix}_all_modulations_accuracy.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 3. Enhanced heatmap visualization\n",
    "    plt.figure(figsize=(16, 8))\n",
    "\n",
    "    # Reorder columns (SNRs) numerically\n",
    "    snr_columns = sorted(accuracy_df.columns, key=int)\n",
    "    accuracy_df_sorted = accuracy_df[snr_columns]\n",
    "\n",
    "    # Create heatmap with better formatting\n",
    "    mask = accuracy_df_sorted.isna()\n",
    "    sns.heatmap(accuracy_df_sorted.astype(float),\n",
    "                annot=True,\n",
    "                cmap='RdYlGn',\n",
    "                fmt='.1f',\n",
    "                mask=mask,\n",
    "                cbar_kws={'label': 'Accuracy (%)'},\n",
    "                linewidths=0.5)\n",
    "\n",
    "    plt.title('Classification Accuracy Heatmap by Modulation and SNR', fontsize=16)\n",
    "    plt.xlabel('Signal-to-Noise Ratio (dB)', fontsize=14)\n",
    "    plt.ylabel('Modulation Type', fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{save_prefix}_modulation_accuracy_heatmap.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # 4. Summary statistics\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"PERFORMANCE SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    overall_acc = np.nanmean(accuracy_df_sorted.values)\n",
    "    print(f\"Overall average accuracy: {overall_acc:.2f}%\")\n",
    "\n",
    "    # Best and worst performing modulations\n",
    "    mod_avg_acc = accuracy_df_sorted.mean(axis=1).sort_values(ascending=False)\n",
    "    print(f\"\\nBest performing modulation: {mod_avg_acc.index[0]} ({mod_avg_acc.iloc[0]:.2f}%)\")\n",
    "    print(f\"Worst performing modulation: {mod_avg_acc.index[-1]} ({mod_avg_acc.iloc[-1]:.2f}%)\")\n",
    "\n",
    "    return accuracy_df_sorted\n",
    "\n",
    "def plot_training_history(model_name, history):\n",
    "    \"\"\"\n",
    "    Enhanced training history plotting with more details\n",
    "    \"\"\"\n",
    "    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(15, 10))\n",
    "\n",
    "    epochs = range(1, len(history['train_loss']) + 1)\n",
    "\n",
    "    # Loss plot\n",
    "    ax1.plot(epochs, history['train_loss'], 'b-', label='Training Loss', linewidth=2)\n",
    "    ax1.plot(epochs, history['val_loss'], 'r-', label='Validation Loss', linewidth=2)\n",
    "    ax1.set_title('Model Loss')\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "\n",
    "    # Accuracy plot\n",
    "    ax2.plot(epochs, [acc * 100 for acc in history['train_acc']], 'b-', label='Training Accuracy', linewidth=2)\n",
    "    ax2.plot(epochs, [acc * 100 for acc in history['val_acc']], 'r-', label='Validation Accuracy', linewidth=2)\n",
    "    ax2.set_title('Model Accuracy')\n",
    "    ax2.set_xlabel('Epoch')\n",
    "    ax2.set_ylabel('Accuracy (%)')\n",
    "    ax2.legend()\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "\n",
    "    # Overfitting analysis\n",
    "    train_val_gap = [abs(t - v) * 100 for t, v in zip(history['train_acc'], history['val_acc'])]\n",
    "    ax3.plot(epochs, train_val_gap, 'g-', linewidth=2)\n",
    "    ax3.set_title('Train-Validation Gap (Overfitting Indicator)')\n",
    "    ax3.set_xlabel('Epoch')\n",
    "    ax3.set_ylabel('Accuracy Gap (%)')\n",
    "    ax3.grid(True, alpha=0.3)\n",
    "\n",
    "    # Show validation loss trend\n",
    "    ax4.plot(epochs, history['val_loss'], 'purple', linewidth=2)\n",
    "    ax4.set_title('Validation Loss Trend')\n",
    "    ax4.set_xlabel('Epoch')\n",
    "    ax4.set_ylabel('Validation Loss')\n",
    "    ax4.grid(True, alpha=0.3)\n",
    "\n",
    "    plt.suptitle(f'Training Analysis: {model_name}', fontsize=16)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{model_name}_detailed_training_history.png', dpi=300, bbox_inches='tight')\n",
    "    plt.show()\n",
    "\n",
    "    # Print training summary\n",
    "    print(f\"\\nTraining Summary for {model_name}:\")\n",
    "    print(f\"Epochs trained: {len(epochs)}\")\n",
    "    print(f\"Final training accuracy: {history['train_acc'][-1]*100:.2f}%\")\n",
    "    print(f\"Final validation accuracy: {history['val_acc'][-1]*100:.2f}%\")\n",
    "    if 'best_val_acc' in history:\n",
    "        print(f\"Best validation accuracy: {history['best_val_acc']*100:.2f}%\")\n",
    "\n",
    "def check_dataset_distribution(dataset_mode='test'):\n",
    "    \"\"\"\n",
    "    Enhanced dataset analysis with better statistics\n",
    "    \"\"\"\n",
    "    # Create dataset for analysis\n",
    "    dataset = RadioML18Dataset(mode=dataset_mode)\n",
    "    test_dl_analysis = DataLoader(dataset=dataset, batch_size=128, shuffle=False)\n",
    "\n",
    "    # Analyze distribution\n",
    "    mod_counts = {}\n",
    "    snr_mod_counts = {}\n",
    "\n",
    "    # Initialize counts for all modulations\n",
    "    for mod in dataset.target_modulations:\n",
    "        mod_counts[mod] = 0\n",
    "\n",
    "    # Set up progress bar\n",
    "    progress_bar = tqdm(range(len(dataset)), desc=f\"Analyzing {dataset_mode} dataset\", leave=True)\n",
    "\n",
    "    # Count occurrences of each modulation\n",
    "    for i in progress_bar:\n",
    "        _, mod_idx, snr = dataset[i]\n",
    "        mod = dataset.target_modulations[mod_idx]\n",
    "\n",
    "        # Count by modulation\n",
    "        mod_counts[mod] += 1\n",
    "\n",
    "        # Count by modulation and SNR\n",
    "        if snr not in snr_mod_counts:\n",
    "            snr_mod_counts[snr] = {}\n",
    "        if mod not in snr_mod_counts[snr]:\n",
    "            snr_mod_counts[snr][mod] = 0\n",
    "        snr_mod_counts[snr][mod] += 1\n",
    "\n",
    "        # Update progress bar less frequently for performance\n",
    "        if i % 1000 == 0:\n",
    "            progress_bar.set_postfix({\"current_mod\": mod, \"snr\": snr})\n",
    "\n",
    "    print(f\"\\n{dataset_mode.upper()} Dataset Distribution Analysis:\")\n",
    "    print(\"=\"*50)\n",
    "    total_samples = sum(mod_counts.values())\n",
    "    print(f\"Total samples: {total_samples:,}\")\n",
    "\n",
    "    print(\"\\nModulation distribution:\")\n",
    "    for mod, count in mod_counts.items():\n",
    "        percentage = (count / total_samples) * 100\n",
    "        print(f\"  {mod}: {count:,} samples ({percentage:.1f}%)\")\n",
    "\n",
    "    # Check if dataset is balanced\n",
    "    counts = list(mod_counts.values())\n",
    "    is_balanced = max(counts) - min(counts) == 0\n",
    "    print(f\"\\nDataset balance: {'Perfectly balanced' if is_balanced else 'Imbalanced'}\")\n",
    "\n",
    "    return mod_counts, snr_mod_counts\n",
    "\n",
    "def improved_train_test_plots(model, model_name, verbose=True, device='cuda', num_epoch=200):\n",
    "    \"\"\"\n",
    "    Complete training and testing pipeline with enhanced analysis\n",
    "    \"\"\"\n",
    "    print(\"=\"*60)\n",
    "    print(f\"TRAINING AND EVALUATION PIPELINE: {model_name}\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    # First check the dataset distribution\n",
    "    print(\"\\n1. Analyzing dataset distribution...\")\n",
    "    mod_counts, snr_mod_counts = check_dataset_distribution('test')\n",
    "\n",
    "    # Train the model\n",
    "    print(f\"\\n2. Training {model_name}...\")\n",
    "    model, train_history = train_model(model, verbose=verbose, device=device, num_epoch=num_epoch)\n",
    "\n",
    "    # Save the trained model\n",
    "    # Define and create the output directory\n",
    "    output_dir = r\"C:\\workarea\\CNN model\\model\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Full paths for saving\n",
    "    state_dict_path = os.path.join(output_dir, f\"{model_name}_state_dict.pth\")\n",
    "    full_model_path = os.path.join(output_dir, f\"{model_name}_full_model.pth\")\n",
    "\n",
    "    # Save the models\n",
    "    torch.save(model.state_dict(), state_dict_path)\n",
    "    torch.save(model, full_model_path)\n",
    "\n",
    "    print(f\"Model saved as:\\n  - {state_dict_path}\\n  - {full_model_path}\")\n",
    "\n",
    "    # Plot training history\n",
    "    print(\"\\n3. Plotting training history...\")\n",
    "    plot_training_history(model_name, train_history)\n",
    "\n",
    "    # Test and analyze the model\n",
    "    print(\"\\n4. Testing model and generating analysis...\")\n",
    "    accuracy_results = plot_improved_test_accuracy(model, device, model_name)\n",
    "\n",
    "    print(\"\\n5. Analysis complete!\")\n",
    "    print(\"=\"*60)\n",
    "\n",
    "    return model, train_history, accuracy_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "f9ceb6f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING AND EVALUATION PIPELINE: ImprovedCNN_NET\n",
      "============================================================\n",
      "\n",
      "1. Analyzing dataset distribution...\n",
      "ðŸ”§ Applying I/Q swap fix to test dataset...\n",
      "âœ… I/Q channels corrected for real-world compatibility\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Analyzing test dataset: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1393392/1393392 [00:06<00:00, 204129.95it/s, current_mod=64QAM, snr=30]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TEST Dataset Distribution Analysis:\n",
      "==================================================\n",
      "Total samples: 1,393,392\n",
      "\n",
      "Modulation distribution:\n",
      "  OOK: 99,528 samples (7.1%)\n",
      "  4ASK: 99,528 samples (7.1%)\n",
      "  8ASK: 99,528 samples (7.1%)\n",
      "  BPSK: 99,528 samples (7.1%)\n",
      "  QPSK: 99,528 samples (7.1%)\n",
      "  8PSK: 99,528 samples (7.1%)\n",
      "  16PSK: 99,528 samples (7.1%)\n",
      "  32PSK: 99,528 samples (7.1%)\n",
      "  16APSK: 99,528 samples (7.1%)\n",
      "  32APSK: 99,528 samples (7.1%)\n",
      "  64APSK: 99,528 samples (7.1%)\n",
      "  16QAM: 99,528 samples (7.1%)\n",
      "  32QAM: 99,528 samples (7.1%)\n",
      "  64QAM: 99,528 samples (7.1%)\n",
      "\n",
      "Dataset balance: Perfectly balanced\n",
      "\n",
      "2. Training ImprovedCNN_NET...\n",
      "GPU: NVIDIA GeForce RTX 5070 Ti\n",
      "Initial GPU memory: 0.05GB / 0.09GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   2%|â–         | 1/50 [00:07<06:06,  7.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 001 | Train Loss: 2.1961, Acc: 0.2303\n",
      "            | Val   Loss: 2.8304, Acc: 0.1793\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.07GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   4%|â–         | 2/50 [00:14<05:56,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 002 | Train Loss: 1.9762, Acc: 0.3187\n",
      "            | Val   Loss: 2.4012, Acc: 0.1991\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   6%|â–Œ         | 3/50 [00:22<05:43,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 003 | Train Loss: 1.9149, Acc: 0.3498\n",
      "            | Val   Loss: 2.2216, Acc: 0.2533\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   8%|â–Š         | 4/50 [00:28<05:28,  7.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 004 | Train Loss: 1.8588, Acc: 0.3757\n",
      "            | Val   Loss: 2.1374, Acc: 0.2825\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  10%|â–ˆ         | 5/50 [00:36<05:29,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 005 | Train Loss: 1.8191, Acc: 0.3965\n",
      "            | Val   Loss: 2.0594, Acc: 0.3250\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  12%|â–ˆâ–        | 6/50 [00:43<05:19,  7.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 006 | Train Loss: 1.7854, Acc: 0.4116\n",
      "            | Val   Loss: 1.9736, Acc: 0.3549\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  14%|â–ˆâ–        | 7/50 [00:50<05:10,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 007 | Train Loss: 1.7625, Acc: 0.4220\n",
      "            | Val   Loss: 1.9694, Acc: 0.3545\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  16%|â–ˆâ–Œ        | 8/50 [00:58<05:05,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 008 | Train Loss: 1.7439, Acc: 0.4304\n",
      "            | Val   Loss: 1.9039, Acc: 0.3754\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  18%|â–ˆâ–Š        | 9/50 [01:05<04:58,  7.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 009 | Train Loss: 1.7238, Acc: 0.4399\n",
      "            | Val   Loss: 1.8271, Acc: 0.3932\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  20%|â–ˆâ–ˆ        | 10/50 [01:12<04:48,  7.22s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 010 | Train Loss: 1.7105, Acc: 0.4460\n",
      "            | Val   Loss: 1.7916, Acc: 0.4067\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n",
      "GPU memory after cleanup: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  22%|â–ˆâ–ˆâ–       | 11/50 [01:20<04:43,  7.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 011 | Train Loss: 1.6988, Acc: 0.4491\n",
      "            | Val   Loss: 1.7704, Acc: 0.4216\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  26%|â–ˆâ–ˆâ–Œ       | 13/50 [01:34<04:30,  7.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 013 | Train Loss: 1.6820, Acc: 0.4591\n",
      "            | Val   Loss: 1.7290, Acc: 0.4369\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  30%|â–ˆâ–ˆâ–ˆ       | 15/50 [01:48<04:11,  7.18s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 015 | Train Loss: 1.6692, Acc: 0.4649\n",
      "            | Val   Loss: 1.7176, Acc: 0.4439\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  32%|â–ˆâ–ˆâ–ˆâ–      | 16/50 [01:55<04:03,  7.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 016 | Train Loss: 1.6623, Acc: 0.4678\n",
      "            | Val   Loss: 1.6972, Acc: 0.4561\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  38%|â–ˆâ–ˆâ–ˆâ–Š      | 19/50 [02:17<03:44,  7.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 019 | Train Loss: 1.6474, Acc: 0.4760\n",
      "            | Val   Loss: 1.6631, Acc: 0.4686\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  40%|â–ˆâ–ˆâ–ˆâ–ˆ      | 20/50 [02:25<03:44,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 020 | Train Loss: 1.6375, Acc: 0.4821\n",
      "            | Val   Loss: 1.6688, Acc: 0.4662\n",
      "            | LR: 0.000100, Patience: 1/20\n",
      "            | GPU Memory: 0.05GB\n",
      "GPU memory after cleanup: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  44%|â–ˆâ–ˆâ–ˆâ–ˆâ–     | 22/50 [02:41<03:32,  7.60s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 022 | Train Loss: 1.6231, Acc: 0.4901\n",
      "            | Val   Loss: 1.6417, Acc: 0.4819\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  48%|â–ˆâ–ˆâ–ˆâ–ˆâ–Š     | 24/50 [02:55<03:12,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 024 | Train Loss: 1.6129, Acc: 0.4956\n",
      "            | Val   Loss: 1.6114, Acc: 0.4940\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 25/50 [03:02<03:03,  7.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 025 | Train Loss: 1.6068, Acc: 0.5003\n",
      "            | Val   Loss: 1.6069, Acc: 0.4954\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  52%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–    | 26/50 [03:10<02:55,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 026 | Train Loss: 1.6011, Acc: 0.5010\n",
      "            | Val   Loss: 1.5997, Acc: 0.4997\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  56%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ    | 28/50 [03:24<02:41,  7.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 028 | Train Loss: 1.5934, Acc: 0.5063\n",
      "            | Val   Loss: 1.5911, Acc: 0.4989\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  60%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ    | 30/50 [03:39<02:23,  7.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 030 | Train Loss: 1.5871, Acc: 0.5082\n",
      "            | Val   Loss: 1.5757, Acc: 0.5068\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n",
      "GPU memory after cleanup: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  64%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–   | 32/50 [03:54<02:12,  7.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 032 | Train Loss: 1.5804, Acc: 0.5114\n",
      "            | Val   Loss: 1.5640, Acc: 0.5124\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  66%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ   | 33/50 [04:01<02:04,  7.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 033 | Train Loss: 1.5771, Acc: 0.5144\n",
      "            | Val   Loss: 1.5668, Acc: 0.5125\n",
      "            | LR: 0.000100, Patience: 1/20\n",
      "            | GPU Memory: 0.05GB\n",
      "            | *** New best validation accuracy! ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  68%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š   | 34/50 [04:08<01:59,  7.44s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 034 | Train Loss: 1.5754, Acc: 0.5153\n",
      "            | Val   Loss: 1.5638, Acc: 0.5145\n",
      "            | LR: 0.000100, Patience: 2/20\n",
      "            | GPU Memory: 0.05GB\n",
      "            | *** New best validation accuracy! ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  70%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ   | 35/50 [04:17<01:58,  7.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 035 | Train Loss: 1.5715, Acc: 0.5170\n",
      "            | Val   Loss: 1.5616, Acc: 0.5149\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  72%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–  | 36/50 [04:26<01:53,  8.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 036 | Train Loss: 1.5690, Acc: 0.5200\n",
      "            | Val   Loss: 1.5634, Acc: 0.5184\n",
      "            | LR: 0.000100, Patience: 1/20\n",
      "            | GPU Memory: 0.05GB\n",
      "            | *** New best validation accuracy! ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  76%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ  | 38/50 [04:43<01:40,  8.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 038 | Train Loss: 1.5612, Acc: 0.5235\n",
      "            | Val   Loss: 1.5727, Acc: 0.5174\n",
      "            | LR: 0.000100, Patience: 3/20\n",
      "            | GPU Memory: 0.05GB\n",
      "            | *** New best validation accuracy! ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  78%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š  | 39/50 [04:51<01:31,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 039 | Train Loss: 1.5593, Acc: 0.5244\n",
      "            | Val   Loss: 1.5430, Acc: 0.5261\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  80%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ  | 40/50 [05:00<01:22,  8.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 040 | Train Loss: 1.5569, Acc: 0.5263\n",
      "            | Val   Loss: 1.5325, Acc: 0.5326\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n",
      "GPU memory after cleanup: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  84%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ– | 42/50 [05:16<01:06,  8.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 042 | Train Loss: 1.5483, Acc: 0.5324\n",
      "            | Val   Loss: 1.5323, Acc: 0.5346\n",
      "            | LR: 0.000100, Patience: 2/20\n",
      "            | GPU Memory: 0.05GB\n",
      "            | *** New best validation accuracy! ***\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  86%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ | 43/50 [05:25<00:58,  8.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 043 | Train Loss: 1.5440, Acc: 0.5345\n",
      "            | Val   Loss: 1.5298, Acc: 0.5388\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  88%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š | 44/50 [05:32<00:47,  7.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 044 | Train Loss: 1.5418, Acc: 0.5356\n",
      "            | Val   Loss: 1.5179, Acc: 0.5385\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  94%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–| 47/50 [05:54<00:22,  7.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 047 | Train Loss: 1.5258, Acc: 0.5439\n",
      "            | Val   Loss: 1.5152, Acc: 0.5431\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  96%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Œ| 48/50 [06:01<00:14,  7.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 048 | Train Loss: 1.5233, Acc: 0.5470\n",
      "            | Val   Loss: 1.5016, Acc: 0.5508\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:  98%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–Š| 49/50 [06:08<00:07,  7.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 049 | Train Loss: 1.5191, Acc: 0.5487\n",
      "            | Val   Loss: 1.4955, Acc: 0.5489\n",
      "            | LR: 0.000100, Patience: 0/20\n",
      "            | GPU Memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 50/50 [06:16<00:00,  7.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 050 | Train Loss: 1.5165, Acc: 0.5485\n",
      "            | Val   Loss: 1.5031, Acc: 0.5462\n",
      "            | LR: 0.000100, Patience: 1/20\n",
      "            | GPU Memory: 0.05GB\n",
      "\n",
      "Training completed!\n",
      "Epochs trained: 50\n",
      "Final validation accuracy: 0.5462\n",
      "Best validation accuracy: 0.5489\n",
      "Final GPU memory: 0.05GB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "Can't get local object 'create_improved_model.<locals>.ImprovedCNN_NET'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 4\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mwarnings\u001b[39;00m\n\u001b[32m      2\u001b[39m warnings.filterwarnings(\u001b[33m'\u001b[39m\u001b[33mignore\u001b[39m\u001b[33m'\u001b[39m, category=\u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[32m----> \u001b[39m\u001b[32m4\u001b[39m \u001b[43mimproved_train_test_plots\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcreate_improved_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn_labels\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m14\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmodel_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mImprovedCNN_NET\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m      9\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_epoch\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m50\u001b[39;49m\u001b[43m \u001b[49m\n\u001b[32m     10\u001b[39m \n\u001b[32m     11\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 359\u001b[39m, in \u001b[36mimproved_train_test_plots\u001b[39m\u001b[34m(model, model_name, verbose, device, num_epoch)\u001b[39m\n\u001b[32m    357\u001b[39m \u001b[38;5;66;03m# Save the models\u001b[39;00m\n\u001b[32m    358\u001b[39m torch.save(model.state_dict(), state_dict_path)\n\u001b[32m--> \u001b[39m\u001b[32m359\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfull_model_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel saved as:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstate_dict_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m  - \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfull_model_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    363\u001b[39m \u001b[38;5;66;03m# Plot training history\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\N208\\.conda\\envs\\amc_env\\Lib\\site-packages\\torch\\serialization.py:965\u001b[39m, in \u001b[36msave\u001b[39m\u001b[34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization, _disable_byteorder_record)\u001b[39m\n\u001b[32m    963\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[32m    964\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[32m--> \u001b[39m\u001b[32m965\u001b[39m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    966\u001b[39m \u001b[43m            \u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    967\u001b[39m \u001b[43m            \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    968\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    969\u001b[39m \u001b[43m            \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    970\u001b[39m \u001b[43m            \u001b[49m\u001b[43m_disable_byteorder_record\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    971\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    972\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m    973\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\N208\\.conda\\envs\\amc_env\\Lib\\site-packages\\torch\\serialization.py:1211\u001b[39m, in \u001b[36m_save\u001b[39m\u001b[34m(obj, zip_file, pickle_module, pickle_protocol, _disable_byteorder_record)\u001b[39m\n\u001b[32m   1208\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m persistent_id(obj)\n\u001b[32m   1210\u001b[39m pickler = PyTorchPickler(data_buf, protocol=pickle_protocol)\n\u001b[32m-> \u001b[39m\u001b[32m1211\u001b[39m \u001b[43mpickler\u001b[49m\u001b[43m.\u001b[49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1212\u001b[39m data_value = data_buf.getvalue()\n\u001b[32m   1213\u001b[39m zip_file.write_record(\u001b[33m\"\u001b[39m\u001b[33mdata.pkl\u001b[39m\u001b[33m\"\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[31mAttributeError\u001b[39m: Can't get local object 'create_improved_model.<locals>.ImprovedCNN_NET'"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=FutureWarning)\n",
    "\n",
    "improved_train_test_plots(\n",
    "    model=create_improved_model(n_labels=14),\n",
    "    model_name='ImprovedCNN_NET',\n",
    "    device='cuda',\n",
    "    verbose= True,\n",
    "    num_epoch=50 \n",
    "\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "558c53a7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amc_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
